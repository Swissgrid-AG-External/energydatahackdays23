{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da127388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import holidays\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26396307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d95dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_season(month, hemisphere):\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9972fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../train_group2/merged_data.csv', skiprows = 0)\n",
    "\n",
    "#data['Unnamed: 0'] = pd.to_datetime(data['Unnamed: 0'])\n",
    "#data.set_index(data.columns[0], inplace=True)\n",
    "#data.index.name = 'Time'\n",
    "\n",
    "data = data.rename(columns={'Unnamed: 0': 'datetime'})\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "\n",
    "# Drop the first column\n",
    "data = data.drop(data.columns[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e20e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43668b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['datetime'].dt.month # Which month of the corresponding year\n",
    "data['day_of_month'] = data['datetime'].dt.day # Which day of the corresponding month\n",
    "#data['day_of_year'] = data['datetime'].dt.dayofyear # Which day of the corresponding year\n",
    "#data['week_of_year'] = data['datetime'].dt.weekofyear # Which week of the corresponding year\n",
    "data['day_of_week'] = data['datetime'].dt.dayofweek # Which day of the corresponding week of the each month\n",
    "data[\"quarter\"] = data['datetime'].dt.quarter\n",
    "data[\"is_wknd\"] = data['datetime'].dt.weekday // 4 # df.date.dt.weekday => Starts from '0' means '0' = 'Monday'. So, '// 4' will give '1' when day number equals\n",
    "# to '5'(which corresponds 'Saturday') and '6'(which corresponds 'Sunday') and '0' for rest of them. Consequently this column will represent whether \n",
    "# the day is weekend or not\n",
    "data['is_month_start'] = data['datetime'].dt.is_month_start.astype(int) # Is it starting of the corresponding month\n",
    "data['is_month_end'] = data['datetime'].dt.is_month_end.astype(int)\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['var'] = data['MWh'].rolling(window=24*5).var()\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Northern'\n",
    "for month in data['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "data['season'] = season_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd477e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39b3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03f4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {\n",
    "    'CH': 'Switzerland',\n",
    "    'IT': 'Italy',\n",
    "    'FR': 'France',\n",
    "    'GER': 'Germany',\n",
    "}\n",
    "\n",
    "# For each country, add a new column with holiday names or NaN if not a holiday\n",
    "for code, country in countries.items():\n",
    "    holiday_dict = holidays.CountryHoliday(country, years=data['datetime'].dt.year.unique().tolist())\n",
    "    data[country] = data['datetime'].apply(lambda x: 1 if x in holiday_dict else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7eb0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on 'month' and 'weekday'\n",
    "\n",
    "df_encoded = pd.get_dummies(data, columns=['Switzerland','Italy', 'France','Germany' ,'month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season' ], prefix=['Switzerland','Italy', 'France','Germany' ,'month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season'])\n",
    "\n",
    "# remove datetime\n",
    "df_encoded = df_encoded.drop('datetime', axis=1)\n",
    "\n",
    "\n",
    "#for col in ['month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season' ]:\n",
    "#    df_encoded[col] = df_encoded[col].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07621578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374cf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7939545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_390886/3602654270.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_seq.iloc[input_seq_length + output_seq_length -1-24 :input_seq_length + output_seq_length - 1]['MWh'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'object':\n",
    "        # Assuming the column's values can be converted to boolean\n",
    "        df_encoded[col] = df_encoded[col].astype(bool)\n",
    "\n",
    "data = df_encoded\n",
    "\n",
    "# Define sequence lengths\n",
    "input_seq_length = 7 * 24  # Previous 7 days\n",
    "output_seq_length = 24     # Next 24 hours\n",
    "\n",
    "# Initialize lists to store sequences\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "# Iterate over the data to create sequences\n",
    "for i in range(0, len(data) - input_seq_length - output_seq_length + 1, output_seq_length):\n",
    "    input_seq = data.iloc[i : i + input_seq_length + output_seq_length]\n",
    "    input_seq.iloc[input_seq_length + output_seq_length -1-24 :input_seq_length + output_seq_length - 1]['MWh'] = np.nan\n",
    "    output_seq = data.iloc[i + input_seq_length : i + input_seq_length + output_seq_length]['MWh']\n",
    "    \n",
    "    # Append the sequences to the lists\n",
    "    input_sequences.append(input_seq)\n",
    "    output_sequences.append(output_seq)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_sequences = np.array(output_sequences)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba55e08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1089, 192, 91)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011b443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1089, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1af1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler_data = MinMaxScaler()\n",
    "input_data_reshaped = input_sequences.reshape(-1, 1)  # Scaler expects 2D data\n",
    "normalized_input_data = min_max_scaler_data.fit_transform(input_data_reshaped)\n",
    "normalized_input_data = normalized_input_data.reshape(input_sequences.shape)\n",
    "\n",
    "min_max_scaler_labels = MinMaxScaler()\n",
    "normalized_labels = min_max_scaler_labels.fit_transform(output_sequences)\n",
    "\n",
    "input_sequences = normalized_input_data\n",
    "output_sequences = normalized_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39af3157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert NumPy arrays to PyTorch tensors\\ninput_train_tensor = torch.tensor(input_train, dtype=torch.float32)\\noutput_train_tensor = torch.tensor(output_train, dtype=torch.float32)\\ninput_test_tensor = torch.tensor(input_test, dtype=torch.float32)\\noutput_test_tensor = torch.tensor(output_test, dtype=torch.float32)\\n\\n# Create PyTorch datasets\\ntrain_dataset = TensorDataset(input_train_tensor, output_train_tensor)\\ntest_dataset = TensorDataset(input_test_tensor, output_test_tensor)\\n\\n# Define batch size\\nbatch_size = 64  # Adjust as needed\\n\\n# Create PyTorch dataloaders\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the index to split the data\n",
    "split_index = int(len(input_sequences) * 0.97)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "input_train = input_sequences[:split_index]\n",
    "\n",
    "output_train = output_sequences[:split_index]\n",
    "input_test = input_sequences[split_index:]\n",
    "output_test = output_sequences[split_index:]\n",
    "\n",
    "# Ensure the data types are compatible with PyTorch tensors\n",
    "input_train = input_train.reshape(input_train.shape[0], input_train.shape[1] * input_train.shape[2])\n",
    "#output_train = output_train.astype(np.float32)\n",
    "input_test = input_test.reshape(input_test.shape[0], input_test.shape[1] * input_test.shape[2])\n",
    "\n",
    "#output_test = output_test.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "input_train_tensor = torch.tensor(input_train, dtype=torch.float32)\n",
    "output_train_tensor = torch.tensor(output_train, dtype=torch.float32)\n",
    "input_test_tensor = torch.tensor(input_test, dtype=torch.float32)\n",
    "output_test_tensor = torch.tensor(output_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(input_train_tensor, output_train_tensor)\n",
    "test_dataset = TensorDataset(input_test_tensor, output_test_tensor)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abd7184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1089, 192, 91)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20adf019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Ensure the data types are compatible with PyTorch tensors\\nall_input = input_sequences.astype(np.float32)\\nall_output = output_sequences.astype(np.float32)\\n\\n# Convert NumPy arrays to PyTorch tensors\\nall_input_train_tensor = torch.tensor(all_input, dtype=torch.float32)\\nall_output_train_tensor = torch.tensor(all_output, dtype=torch.float32)\\n\\n# Create PyTorch datasets\\nfull_dataset = TensorDataset(all_input_train_tensor, all_output_train_tensor)\\n\\n# Define batch size\\nbatch_size = 64  # Adjust as needed\\n\\n# Create PyTorch dataloaders\\nfull_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Ensure the data types are compatible with PyTorch tensors\n",
    "all_input = input_sequences.astype(np.float32)\n",
    "all_output = output_sequences.astype(np.float32)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "all_input_train_tensor = torch.tensor(all_input, dtype=torch.float32)\n",
    "all_output_train_tensor = torch.tensor(all_output, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "full_dataset = TensorDataset(all_input_train_tensor, all_output_train_tensor)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0500b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_train[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb25f60",
   "metadata": {},
   "source": [
    "# LightGBM parameters : Actually that parameters is pretrained parameters means this parameters have been found by GridSearch/RandomizedSearch. \n",
    "#So I am writing using it directly, normally it should have been found by GridSearch/RandomizedSearch methods. \n",
    "lgb_params = {'metric': {'mae'},\n",
    "              'num_leaves': 10,\n",
    "              'learning_rate': 0.02,\n",
    "              'feature_fraction': 0.8,\n",
    "              'max_depth': 5,\n",
    "              'verbose': 0,\n",
    "              'early_stopping_rounds': 200,\n",
    "             'device': 'gpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1a4df1",
   "metadata": {},
   "source": [
    "pattern = re.compile(r'[,\\[\\]{}\"]')\n",
    "\n",
    "# Check each column name for special characters\n",
    "problematic_columns = [col for col in X_train.columns if pattern.search(col)]\n",
    "\n",
    "print(problematic_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e62a81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\\n\\nmodel = lgb.train(lgb_params, lgbtrain,\\n                  valid_sets=[lgbtrain, lgbval], \\n                 num_boost_round = 11000)\\n                 \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n",
    "lgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n",
    "\n",
    "model = lgb.train(lgb_params, lgbtrain,\n",
    "                  valid_sets=[lgbtrain, lgbval], \n",
    "                 num_boost_round = 11000)\n",
    "                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89251d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = lgb.LGBMRegressor(**lgb_params)\\nmodel.fit(X_train, Y_train, eval_set=[(X_val, Y_val)])\\npredictions = model.predict(X_val)\\nrmse = mean_absolute_error(Y_val, predictions)\\nprint(rmse)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model.fit(X_train, Y_train, eval_set=[(X_val, Y_val)])\n",
    "predictions = model.predict(X_val)\n",
    "rmse = mean_absolute_error(Y_val, predictions)\n",
    "print(rmse)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b329ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# LGBM\\ncv_split = TimeSeriesSplit(n_splits=4, test_size=100)\\nmodel = lgb.LGBMRegressor()\\nparameters = {\\n    \"max_depth\": [3, 4, 6, 5, 10],\\n    \"num_leaves\": [10, 20, 30, 40, 100, 120],\\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\\n    \"n_estimators\": [50, 100, 300, 500, 700, 900, 1000],\\n    \"colsample_bytree\": [0.3, 0.5, 0.7, 1]\\n}\\n\\n\\ngrid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\\ngrid_search.fit(X_train, y_train)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# LGBM\n",
    "cv_split = TimeSeriesSplit(n_splits=4, test_size=100)\n",
    "model = lgb.LGBMRegressor()\n",
    "parameters = {\n",
    "    \"max_depth\": [3, 4, 6, 5, 10],\n",
    "    \"num_leaves\": [10, 20, 30, 40, 100, 120],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"n_estimators\": [50, 100, 300, 500, 700, 900, 1000],\n",
    "    \"colsample_bytree\": [0.3, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422ee22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb521d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235f11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beee62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023449 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.333856\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[299]\tvalid_0's l1: 0.00513297\n",
      "step: 1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.021696 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.322174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[287]\tvalid_0's l1: 0.00414994\n",
      "step: 2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022290 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.292643\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[222]\tvalid_0's l1: 0.00515824\n",
      "step: 3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022304 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.281819\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[203]\tvalid_0's l1: 0.00354663\n",
      "step: 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022562 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.294553\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 0.00474624\n",
      "step: 5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022543 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.269121\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[295]\tvalid_0's l1: 0.00737458\n",
      "step: 6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022337 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.243486\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[296]\tvalid_0's l1: 0.00822267\n",
      "step: 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023047 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.268484\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[246]\tvalid_0's l1: 0.00509294\n",
      "step: 8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022760 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.330885\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00602522\n",
      "step: 9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023045 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.353491\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[226]\tvalid_0's l1: 0.00530761\n",
      "step: 10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022974 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.363317\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[203]\tvalid_0's l1: 0.00776587\n",
      "step: 11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023223 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.352891\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00582709\n",
      "step: 12\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022904 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.338140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[279]\tvalid_0's l1: 0.00491656\n",
      "step: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023282 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.329076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[298]\tvalid_0's l1: 0.00377\n",
      "step: 14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023219 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.370069\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00682612\n",
      "step: 15\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022613 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.393644\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.0104137\n",
      "step: 16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023153 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.381107\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00803353\n",
      "step: 17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023439 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.351972\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's l1: 0.00785215\n",
      "step: 18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023241 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.354659\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's l1: 0.00643182\n",
      "step: 19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023054 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.395223\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00900555\n",
      "step: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023267 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.323532\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[221]\tvalid_0's l1: 0.00494717\n",
      "step: 21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.312626\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[262]\tvalid_0's l1: 0.0079364\n",
      "step: 22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.023117 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.305070\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[203]\tvalid_0's l1: 0.00603176\n",
      "step: 23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 578536\n",
      "[LightGBM] [Info] Number of data points in the train set: 1056, number of used features: 16792\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA A100 80GB PCIe, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 16\n",
      "[LightGBM] [Info] 3704 dense feature groups (3.73 MB) transferred to GPU in 0.022850 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] num_iterations is set=300, num_boost_round=300 will be ignored. Current value: num_iterations=300\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Start training from score 0.310353\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's l1: 0.00737563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgb_params = {\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 10,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'max_depth': 20,\n",
    "    'verbose': 1,  # changed to 1 for verbose logging\n",
    "    'num_boost_round': 300,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'device': 'gpu',\n",
    "    # 'gpu_platform_id': 0,  # Uncomment if necessary\n",
    "    # 'gpu_device_id': 0,    # Uncomment if necessary\n",
    "    'gpu_use_dp': True  # Use double precision\n",
    "}\n",
    "\n",
    "models = {}\n",
    "model_multistep = lgb.LGBMRegressor(**lgb_params)  # Initialize once\n",
    "\n",
    "for i in range(24):\n",
    "    print(\"step:\", i)\n",
    "    \n",
    "    # Assuming input_train, output_train, input_test, and output_test are already defined\n",
    "    model_multistep.fit(input_train, output_train[:, i], eval_set=[(input_test, output_test[:, i])])  # Print every 100 rounds\n",
    "    \n",
    "    models[f'model_{i}'] = model_multistep\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33cde622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saji_models_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca49d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03803828786876852"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(models['model_0'].predict(input_test) - output_test[:, 0]))\n",
    "np.mean(abs(models['model_1'].predict(input_test) - output_test[:, 1]))\n",
    "np.mean(abs(models['model_2'].predict(input_test) - output_test[:, 2]))\n",
    "np.mean(abs(models['model_3'].predict(input_test) - output_test[:, 3]))\n",
    "np.mean(abs(models['model_4'].predict(input_test) - output_test[:, 4]))\n",
    "np.mean(abs(models['model_5'].predict(input_test) - output_test[:, 5]))\n",
    "np.mean(abs(models['model_6'].predict(input_test) - output_test[:, 6]))\n",
    "\n",
    "np.mean(abs(models['model_7'].predict(input_test) - output_test[:, 7]))\n",
    "np.mean(abs(models['model_8'].predict(input_test) - output_test[:, 8]))\n",
    "np.mean(abs(models['model_9'].predict(input_test) - output_test[:, 9]))\n",
    "np.mean(abs(models['model_10'].predict(input_test) - output_test[:, 10]))\n",
    "np.mean(abs(models['model_11'].predict(input_test) - output_test[:, 11]))\n",
    "np.mean(abs(models['model_12'].predict(input_test) - output_test[:, 12]))\n",
    "np.mean(abs(models['model_13'].predict(input_test) - output_test[:, 13]))\n",
    "\n",
    "np.mean(abs(models['model_14'].predict(input_test) - output_test[:, 14]))\n",
    "np.mean(abs(models['model_15'].predict(input_test) - output_test[:, 15]))\n",
    "np.mean(abs(models['model_16'].predict(input_test) - output_test[:, 16]))\n",
    "np.mean(abs(models['model_17'].predict(input_test) - output_test[:, 17]))\n",
    "np.mean(abs(models['model_18'].predict(input_test) - output_test[:, 18]))\n",
    "np.mean(abs(models['model_19'].predict(input_test) - output_test[:, 19]))\n",
    "np.mean(abs(models['model_20'].predict(input_test) - output_test[:, 20]))\n",
    "\n",
    "np.mean(abs(models['model_21'].predict(input_test) - output_test[:, 21]))\n",
    "np.mean(abs(models['model_22'].predict(input_test) - output_test[:, 22]))\n",
    "np.mean(abs(models['model_23'].predict(input_test) - output_test[:,22]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c987d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 24)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "788538c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[116.60924408, 126.9984372 , 125.95285102, ..., 126.81711829,\n",
       "        107.85424371,  89.2776395 ],\n",
       "       [ 82.13964682,  82.59810723,  93.61280993, ...,  89.39895988,\n",
       "         83.58139282,  93.70273729],\n",
       "       [ 99.32936341,  95.81622047,  90.21143037, ...,  67.05376591,\n",
       "         64.2172988 ,  66.06269023],\n",
       "       ...,\n",
       "       [124.12143695, 115.54929113, 108.69566134, ..., 205.33900331,\n",
       "        198.81006446, 177.54828634],\n",
       "       [133.9305803 , 132.4915909 , 136.87537298, ..., 193.91019451,\n",
       "        189.60759661, 161.07037915],\n",
       "       [176.1359428 , 193.20642619, 198.78469971, ..., 155.10951973,\n",
       "        171.37027694, 146.05479082]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f352b871",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msingle_prediction\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "single_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "180d0784",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_errors(models, input_test, output_test):\n",
    "    errors = []\n",
    "    aggregated_predictions = np.zeros((input_test.shape[0], 24))  # Assuming input_test.shape[0] = 33\n",
    "\n",
    "    for i in range(24):\n",
    "        predictions = models[f'model_{i}'].predict(input_test)\n",
    "        aggregated_predictions[:, i] = predictions\n",
    "        \n",
    "        \n",
    "        \n",
    "        error = np.mean(abs(predictions - output_test[:, i]))\n",
    "        errors.append(error)\n",
    "\n",
    "    return np.mean(errors), aggregated_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c18978c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.10414416255407417\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.09819737510275117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.08306418273447422\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.08314739722347197\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.09052900921557827\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.07919659010578807\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.10045028736126212\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.09964772166593529\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.11362301858471979\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.1329511191499209\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.12490152992379663\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.10773625369707363\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.08452326873909419\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.07726292014405133\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.10295339187017973\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.14921970159899892\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.15178104024320874\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.13042043415222102\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.11241830962441128\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.1263270734508985\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.06598104460134947\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.057712361751033454\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.03803828786876852\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "(33,)\n",
      "0.007375633467554076\n",
      "Overall mean of the MAEs: 0.09673342145127566\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_errors(models, input_test, output_test):\n",
    "    errors = []\n",
    "    for i in range(24):\n",
    "        predictions = models[f'model_{i}'].predict(input_test)\n",
    "        print(predictions.shape)\n",
    "        error = np.mean(abs(predictions - output_test[:, i]))\n",
    "        print(error)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors)\n",
    "\n",
    "overall_mean = mean_absolute_errors(models, input_test, output_test)\n",
    "print(f\"Overall mean of the MAEs: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a9949d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here location to your test data and modell\n",
    "activelosses = \"../results_group2/Active-Losses-2022_test.csv\"\n",
    "renewablegen = \"../results_group2/Forecast-renewable-generation_2022_test.csv\"\n",
    "forecasttemp = '../results_group2/eq_temp_2022_test.csv.csv'\n",
    "ntc = '../results_group2/ntc_2022_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bf207a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MWh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>146.054792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>139.133354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>147.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>157.636204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>163.326766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00</th>\n",
       "      <td>67.876028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00</th>\n",
       "      <td>72.765318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00</th>\n",
       "      <td>81.277633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00</th>\n",
       "      <td>95.496046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00</th>\n",
       "      <td>74.066704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            MWh\n",
       "datetime                       \n",
       "2022-01-01 00:00:00  146.054792\n",
       "2022-01-01 01:00:00  139.133354\n",
       "2022-01-01 02:00:00  147.562500\n",
       "2022-01-01 03:00:00  157.636204\n",
       "2022-01-01 04:00:00  163.326766\n",
       "...                         ...\n",
       "2022-12-31 19:00:00   67.876028\n",
       "2022-12-31 20:00:00   72.765318\n",
       "2022-12-31 21:00:00   81.277633\n",
       "2022-12-31 22:00:00   95.496046\n",
       "2022-12-31 23:00:00   74.066704\n",
       "\n",
       "[8760 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeslosses = pd.read_csv(activelosses)\n",
    "activeslosses.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n",
    "activeslosses = activeslosses.iloc[:, 2:]\n",
    "# Rename column 'A' to 'X'\n",
    "activeslosses['datetime'] = pd.to_datetime(activeslosses['datetime'])\n",
    "activeslosses.rename(columns={'Wirkverluste/Active Losses': 'MWh'}, inplace=True)\n",
    "activeslosses.set_index(activeslosses.columns[0], inplace=True)\n",
    "activeslosses = activeslosses[~activeslosses.index.duplicated(keep='first')]\n",
    "activeslosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d95ec305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solar_fore_de_mw</th>\n",
       "      <th>solar_fore_it_mw</th>\n",
       "      <th>wind_fore_de_mw</th>\n",
       "      <th>wind_fore_it_mw</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31805.65</td>\n",
       "      <td>1331.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29880.67</td>\n",
       "      <td>1438.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28826.75</td>\n",
       "      <td>1623.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27631.75</td>\n",
       "      <td>1894.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27128.00</td>\n",
       "      <td>2335.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44107.32</td>\n",
       "      <td>569.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44512.60</td>\n",
       "      <td>459.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44946.45</td>\n",
       "      <td>399.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44938.83</td>\n",
       "      <td>420.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44693.53</td>\n",
       "      <td>501.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     solar_fore_de_mw  solar_fore_it_mw  wind_fore_de_mw  \\\n",
       "datetime                                                                   \n",
       "2022-01-01 00:00:00               0.0               0.0         31805.65   \n",
       "2022-01-01 01:00:00               0.0               0.0         29880.67   \n",
       "2022-01-01 02:00:00               0.0               0.0         28826.75   \n",
       "2022-01-01 03:00:00               0.0               0.0         27631.75   \n",
       "2022-01-01 04:00:00               0.0               0.0         27128.00   \n",
       "...                               ...               ...              ...   \n",
       "2022-12-31 19:00:00               0.0               0.0         44107.32   \n",
       "2022-12-31 20:00:00               0.0               0.0         44512.60   \n",
       "2022-12-31 21:00:00               0.0               0.0         44946.45   \n",
       "2022-12-31 22:00:00               0.0               0.0         44938.83   \n",
       "2022-12-31 23:00:00               0.0               0.0         44693.53   \n",
       "\n",
       "                     wind_fore_it_mw  \n",
       "datetime                              \n",
       "2022-01-01 00:00:00          1331.48  \n",
       "2022-01-01 01:00:00          1438.15  \n",
       "2022-01-01 02:00:00          1623.80  \n",
       "2022-01-01 03:00:00          1894.75  \n",
       "2022-01-01 04:00:00          2335.05  \n",
       "...                              ...  \n",
       "2022-12-31 19:00:00           569.68  \n",
       "2022-12-31 20:00:00           459.45  \n",
       "2022-12-31 21:00:00           399.35  \n",
       "2022-12-31 22:00:00           420.78  \n",
       "2022-12-31 23:00:00           501.10  \n",
       "\n",
       "[8760 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forecast_renew = pd.read_csv(renewablegen, skiprows=0)\n",
    "Forecast_renew = Forecast_renew.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n",
    "# Load Forecast Renewable Generation data\n",
    "Forecast_renew['datetime'] = pd.to_datetime(Forecast_renew['datetime'])\n",
    "Forecast_renew.set_index(Forecast_renew.columns[0], inplace=True)\n",
    "Forecast_renew = Forecast_renew[~Forecast_renew.index.duplicated(keep='first')]\n",
    "Forecast_renew = Forecast_renew.resample('H').asfreq()\n",
    "Forecast_renew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9faa7328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_fore_ch</th>\n",
       "      <th>temperature_fore_fr</th>\n",
       "      <th>temperature_fore_de</th>\n",
       "      <th>temperature_fore_it</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>6.65</td>\n",
       "      <td>7.90</td>\n",
       "      <td>10.25</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>6.77</td>\n",
       "      <td>8.31</td>\n",
       "      <td>10.64</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>6.42</td>\n",
       "      <td>8.01</td>\n",
       "      <td>10.46</td>\n",
       "      <td>7.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>6.08</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10.21</td>\n",
       "      <td>6.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>5.68</td>\n",
       "      <td>7.62</td>\n",
       "      <td>10.07</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00</th>\n",
       "      <td>10.37</td>\n",
       "      <td>14.18</td>\n",
       "      <td>14.13</td>\n",
       "      <td>11.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00</th>\n",
       "      <td>9.67</td>\n",
       "      <td>13.90</td>\n",
       "      <td>13.91</td>\n",
       "      <td>10.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00</th>\n",
       "      <td>9.10</td>\n",
       "      <td>13.57</td>\n",
       "      <td>13.79</td>\n",
       "      <td>10.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00</th>\n",
       "      <td>8.64</td>\n",
       "      <td>13.29</td>\n",
       "      <td>13.64</td>\n",
       "      <td>9.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00</th>\n",
       "      <td>8.25</td>\n",
       "      <td>13.20</td>\n",
       "      <td>13.61</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temperature_fore_ch  temperature_fore_fr  \\\n",
       "datetime                                                        \n",
       "2022-01-01 00:00:00                 6.65                 7.90   \n",
       "2022-01-01 01:00:00                 6.77                 8.31   \n",
       "2022-01-01 02:00:00                 6.42                 8.01   \n",
       "2022-01-01 03:00:00                 6.08                 7.77   \n",
       "2022-01-01 04:00:00                 5.68                 7.62   \n",
       "...                                  ...                  ...   \n",
       "2022-12-31 19:00:00                10.37                14.18   \n",
       "2022-12-31 20:00:00                 9.67                13.90   \n",
       "2022-12-31 21:00:00                 9.10                13.57   \n",
       "2022-12-31 22:00:00                 8.64                13.29   \n",
       "2022-12-31 23:00:00                 8.25                13.20   \n",
       "\n",
       "                     temperature_fore_de  temperature_fore_it  \n",
       "datetime                                                       \n",
       "2022-01-01 00:00:00                10.25                 7.38  \n",
       "2022-01-01 01:00:00                10.64                 7.25  \n",
       "2022-01-01 02:00:00                10.46                 7.09  \n",
       "2022-01-01 03:00:00                10.21                 6.94  \n",
       "2022-01-01 04:00:00                10.07                 6.85  \n",
       "...                                  ...                  ...  \n",
       "2022-12-31 19:00:00                14.13                11.68  \n",
       "2022-12-31 20:00:00                13.91                10.76  \n",
       "2022-12-31 21:00:00                13.79                10.06  \n",
       "2022-12-31 22:00:00                13.64                 9.61  \n",
       "2022-12-31 23:00:00                13.61                 9.21  \n",
       "\n",
       "[8759 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forecast_temp = pd.read_csv(forecasttemp, skiprows=0)\n",
    "Forecast_temp = Forecast_temp.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n",
    "Forecast_temp['datetime'] = pd.to_datetime(Forecast_temp['datetime'])\n",
    "Forecast_temp.set_index(Forecast_temp.columns[0], inplace=True)\n",
    "Forecast_temp = Forecast_temp[~Forecast_temp.index.duplicated(keep='first')]\n",
    "Forecast_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16ee3140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CH_AT</th>\n",
       "      <th>CH_DE</th>\n",
       "      <th>CH_FR</th>\n",
       "      <th>CH_IT</th>\n",
       "      <th>AT_CH</th>\n",
       "      <th>DE_CH</th>\n",
       "      <th>FR_CH</th>\n",
       "      <th>IT_CH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01 00:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>3158.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 01:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>3213.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 02:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2824.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 03:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2678.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01 04:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2629.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 20:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 21:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 22:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1459.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 23:00:00</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CH_AT   CH_DE   CH_FR   CH_IT   AT_CH  DE_CH   FR_CH  \\\n",
       "datetime                                                                     \n",
       "2022-01-01 00:00:00  1200.0  4000.0  1400.0  3158.0  1200.0  800.0  3200.0   \n",
       "2022-01-01 01:00:00  1200.0  4000.0  1400.0  3213.0  1200.0  800.0  3200.0   \n",
       "2022-01-01 02:00:00  1200.0  4000.0  1400.0  2824.0  1200.0  800.0  3200.0   \n",
       "2022-01-01 03:00:00  1200.0  4000.0  1400.0  2678.0  1200.0  800.0  3200.0   \n",
       "2022-01-01 04:00:00  1200.0  4000.0  1400.0  2629.0  1200.0  800.0  3200.0   \n",
       "...                     ...     ...     ...     ...     ...    ...     ...   \n",
       "2022-12-31 19:00:00  1200.0  4000.0  1200.0  1507.0  1000.0  800.0  3700.0   \n",
       "2022-12-31 20:00:00  1200.0  4000.0  1200.0  1507.0  1000.0  800.0  3700.0   \n",
       "2022-12-31 21:00:00  1200.0  4000.0  1200.0  1507.0  1000.0  800.0  3700.0   \n",
       "2022-12-31 22:00:00  1200.0  4000.0  1200.0  1459.0  1000.0  800.0  3700.0   \n",
       "2022-12-31 23:00:00  1200.0  4000.0  1200.0  1411.0  1000.0  800.0  3700.0   \n",
       "\n",
       "                      IT_CH  \n",
       "datetime                     \n",
       "2022-01-01 00:00:00  1910.0  \n",
       "2022-01-01 01:00:00  1910.0  \n",
       "2022-01-01 02:00:00  1910.0  \n",
       "2022-01-01 03:00:00  1910.0  \n",
       "2022-01-01 04:00:00  1910.0  \n",
       "...                     ...  \n",
       "2022-12-31 19:00:00  1810.0  \n",
       "2022-12-31 20:00:00  1810.0  \n",
       "2022-12-31 21:00:00  1810.0  \n",
       "2022-12-31 22:00:00  1810.0  \n",
       "2022-12-31 23:00:00  1910.0  \n",
       "\n",
       "[8760 rows x 8 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load NTC data\n",
    "NTC = pd.read_csv(ntc, skiprows=0)\n",
    "NTC = NTC.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n",
    "NTC['datetime'] = pd.to_datetime(NTC['datetime'])\n",
    "NTC.set_index(NTC.columns[0], inplace=True)\n",
    "NTC = NTC[~NTC.index.duplicated(keep='first')]\n",
    "NTC = NTC.resample('H').asfreq()\n",
    "NTC = NTC.interpolate(method='polynomial', order=2)\n",
    "NTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28452651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing steps\n",
    "activeslosses = activeslosses.interpolate(method='polynomial', order=2)\n",
    "activeslosses_hour = activeslosses.resample('H').sum()\n",
    "Forecast_renew = Forecast_renew.interpolate(method='polynomial', order=2)\n",
    "Forecast_temp_hourly = Forecast_temp.resample('H').asfreq()\n",
    "Forecast_temp_hourly_lin = Forecast_temp_hourly.interpolate(method='linear')\n",
    "Forecast_temp_hourly_poly = Forecast_temp_hourly.interpolate(method='polynomial', order=1)\n",
    "NTC = NTC.interpolate(method='polynomial', order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bc8f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "merged_df = pd.merge(activeslosses, Forecast_renew, left_index=True, right_index=True, how='outer')\n",
    "merged_df = pd.merge(merged_df, Forecast_temp, left_index=True, right_index=True, how='outer')\n",
    "merged_df = pd.merge(merged_df, NTC, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a22c833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.reset_index()\n",
    "\n",
    "data = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff010948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7dd65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Unnamed: 0': 'datetime'})\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "\n",
    "# Drop the first column\n",
    "data = data.drop(data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbe9f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['datetime'].dt.month # Which month of the corresponding year\n",
    "data['day_of_month'] = data['datetime'].dt.day # Which day of the corresponding month\n",
    "#data['day_of_year'] = data['datetime'].dt.dayofyear # Which day of the corresponding year\n",
    "#data['week_of_year'] = data['datetime'].dt.weekofyear # Which week of the corresponding year\n",
    "data['day_of_week'] = data['datetime'].dt.dayofweek # Which day of the corresponding week of the each month\n",
    "data[\"quarter\"] = data['datetime'].dt.quarter\n",
    "data[\"is_wknd\"] = data['datetime'].dt.weekday // 4 # df.date.dt.weekday => Starts from '0' means '0' = 'Monday'. So, '// 4' will give '1' when day number equals\n",
    "# to '5'(which corresponds 'Saturday') and '6'(which corresponds 'Sunday') and '0' for rest of them. Consequently this column will represent whether \n",
    "# the day is weekend or not\n",
    "data['is_month_start'] = data['datetime'].dt.is_month_start.astype(int) # Is it starting of the corresponding month\n",
    "data['is_month_end'] = data['datetime'].dt.is_month_end.astype(int)\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['var'] = data['MWh'].rolling(window=24*5).var()\n",
    "\n",
    "season_list = []\n",
    "hemisphere = 'Northern'\n",
    "for month in data['month']:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "data['season'] = season_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e51d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {\n",
    "    'CH': 'Switzerland',\n",
    "    'IT': 'Italy',\n",
    "    'FR': 'France',\n",
    "    'GER': 'Germany',\n",
    "}\n",
    "\n",
    "# For each country, add a new column with holiday names or NaN if not a holiday\n",
    "for code, country in countries.items():\n",
    "    holiday_dict = holidays.CountryHoliday(country, years=data['datetime'].dt.year.unique().tolist())\n",
    "    data[country] = data['datetime'].apply(lambda x: 1 if x in holiday_dict else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aba4243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on 'month' and 'weekday'\n",
    "\n",
    "df_encoded = pd.get_dummies(data, columns=['Switzerland','Italy', 'France','Germany' ,'month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season' ], prefix=['Switzerland','Italy', 'France','Germany' ,'month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season'])\n",
    "\n",
    "# remove datetime\n",
    "df_encoded = df_encoded.drop('datetime', axis=1)\n",
    "\n",
    "\n",
    "#for col in ['month', 'day_of_month', \"day_of_week\", \"quarter\", \"is_wknd\", 'is_month_start','is_month_end', 'season' ]:\n",
    "#    df_encoded[col] = df_encoded[col].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e494274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solar_fore_de_mw</th>\n",
       "      <th>solar_fore_it_mw</th>\n",
       "      <th>wind_fore_de_mw</th>\n",
       "      <th>wind_fore_it_mw</th>\n",
       "      <th>temperature_fore_ch</th>\n",
       "      <th>temperature_fore_fr</th>\n",
       "      <th>temperature_fore_de</th>\n",
       "      <th>temperature_fore_it</th>\n",
       "      <th>CH_AT</th>\n",
       "      <th>CH_DE</th>\n",
       "      <th>...</th>\n",
       "      <th>is_wknd_0</th>\n",
       "      <th>is_wknd_1</th>\n",
       "      <th>is_month_start_0</th>\n",
       "      <th>is_month_start_1</th>\n",
       "      <th>is_month_end_0</th>\n",
       "      <th>is_month_end_1</th>\n",
       "      <th>season_Autumn</th>\n",
       "      <th>season_Spring</th>\n",
       "      <th>season_Summer</th>\n",
       "      <th>season_Winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31805.65</td>\n",
       "      <td>1331.48</td>\n",
       "      <td>6.65</td>\n",
       "      <td>7.90</td>\n",
       "      <td>10.25</td>\n",
       "      <td>7.38</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29880.67</td>\n",
       "      <td>1438.15</td>\n",
       "      <td>6.77</td>\n",
       "      <td>8.31</td>\n",
       "      <td>10.64</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28826.75</td>\n",
       "      <td>1623.80</td>\n",
       "      <td>6.42</td>\n",
       "      <td>8.01</td>\n",
       "      <td>10.46</td>\n",
       "      <td>7.09</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27631.75</td>\n",
       "      <td>1894.75</td>\n",
       "      <td>6.08</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10.21</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27128.00</td>\n",
       "      <td>2335.05</td>\n",
       "      <td>5.68</td>\n",
       "      <td>7.62</td>\n",
       "      <td>10.07</td>\n",
       "      <td>6.85</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32304.12</td>\n",
       "      <td>3411.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33191.38</td>\n",
       "      <td>2991.78</td>\n",
       "      <td>2.07</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33930.55</td>\n",
       "      <td>2702.10</td>\n",
       "      <td>1.92</td>\n",
       "      <td>7.51</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34514.90</td>\n",
       "      <td>2708.43</td>\n",
       "      <td>2.05</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34726.85</td>\n",
       "      <td>2782.75</td>\n",
       "      <td>2.14</td>\n",
       "      <td>6.66</td>\n",
       "      <td>1.10</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows  89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     solar_fore_de_mw  solar_fore_it_mw  wind_fore_de_mw  wind_fore_it_mw  \\\n",
       "0                 0.0               0.0         31805.65          1331.48   \n",
       "1                 0.0               0.0         29880.67          1438.15   \n",
       "2                 0.0               0.0         28826.75          1623.80   \n",
       "3                 0.0               0.0         27631.75          1894.75   \n",
       "4                 0.0               0.0         27128.00          2335.05   \n",
       "..                ...               ...              ...              ...   \n",
       "187               0.0               0.0         32304.12          3411.88   \n",
       "188               0.0               0.0         33191.38          2991.78   \n",
       "189               0.0               0.0         33930.55          2702.10   \n",
       "190               0.0               0.0         34514.90          2708.43   \n",
       "191               0.0               0.0         34726.85          2782.75   \n",
       "\n",
       "     temperature_fore_ch  temperature_fore_fr  temperature_fore_de  \\\n",
       "0                   6.65                 7.90                10.25   \n",
       "1                   6.77                 8.31                10.64   \n",
       "2                   6.42                 8.01                10.46   \n",
       "3                   6.08                 7.77                10.21   \n",
       "4                   5.68                 7.62                10.07   \n",
       "..                   ...                  ...                  ...   \n",
       "187                 2.22                 7.87                 0.78   \n",
       "188                 2.07                 7.78                 0.79   \n",
       "189                 1.92                 7.51                 0.91   \n",
       "190                 2.05                 6.99                 0.99   \n",
       "191                 2.14                 6.66                 1.10   \n",
       "\n",
       "     temperature_fore_it   CH_AT   CH_DE  ...  is_wknd_0  is_wknd_1  \\\n",
       "0                   7.38  1200.0  4000.0  ...      False       True   \n",
       "1                   7.25  1200.0  4000.0  ...      False       True   \n",
       "2                   7.09  1200.0  4000.0  ...      False       True   \n",
       "3                   6.94  1200.0  4000.0  ...      False       True   \n",
       "4                   6.85  1200.0  4000.0  ...      False       True   \n",
       "..                   ...     ...     ...  ...        ...        ...   \n",
       "187                 4.88  1200.0  4000.0  ...      False       True   \n",
       "188                 4.27  1200.0  4000.0  ...      False       True   \n",
       "189                 3.79  1200.0  4000.0  ...      False       True   \n",
       "190                 3.56  1200.0  4000.0  ...      False       True   \n",
       "191                 3.44  1200.0  4000.0  ...      False       True   \n",
       "\n",
       "     is_month_start_0  is_month_start_1  is_month_end_0  is_month_end_1  \\\n",
       "0               False              True            True           False   \n",
       "1               False              True            True           False   \n",
       "2               False              True            True           False   \n",
       "3               False              True            True           False   \n",
       "4               False              True            True           False   \n",
       "..                ...               ...             ...             ...   \n",
       "187              True             False            True           False   \n",
       "188              True             False            True           False   \n",
       "189              True             False            True           False   \n",
       "190              True             False            True           False   \n",
       "191              True             False            True           False   \n",
       "\n",
       "     season_Autumn  season_Spring  season_Summer  season_Winter  \n",
       "0            False          False          False           True  \n",
       "1            False          False          False           True  \n",
       "2            False          False          False           True  \n",
       "3            False          False          False           True  \n",
       "4            False          False          False           True  \n",
       "..             ...            ...            ...            ...  \n",
       "187          False          False          False           True  \n",
       "188          False          False          False           True  \n",
       "189          False          False          False           True  \n",
       "190          False          False          False           True  \n",
       "191          False          False          False           True  \n",
       "\n",
       "[192 rows x 89 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7271c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173472/2733691500.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_seq.iloc[input_seq_length + output_seq_length -1-24 :input_seq_length + output_seq_length - 1]['MWh'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'object':\n",
    "        # Assuming the column's values can be converted to boolean\n",
    "        df_encoded[col] = df_encoded[col].astype(bool)\n",
    "\n",
    "data = df_encoded\n",
    "\n",
    "# Define sequence lengths\n",
    "input_seq_length = 7 * 24  # Previous 7 days\n",
    "output_seq_length = 24     # Next 24 hours\n",
    "\n",
    "# Initialize lists to store sequences\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "# Iterate over the data to create sequences\n",
    "for i in range(0, len(data) - input_seq_length - output_seq_length + 1, output_seq_length):\n",
    "    input_seq = data.iloc[i : i + input_seq_length + output_seq_length]\n",
    "    input_seq.iloc[input_seq_length + output_seq_length -1-24 :input_seq_length + output_seq_length - 1]['MWh'] = np.nan\n",
    "    output_seq = data.iloc[i + input_seq_length : i + input_seq_length + output_seq_length]['MWh']\n",
    "    \n",
    "    # Append the sequences to the lists\n",
    "    input_sequences.append(input_seq)\n",
    "    output_sequences.append(output_seq)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_sequences = np.array(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38bfe875",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_val = input_sequences.reshape(input_sequences.shape[0], input_sequences.shape[1] * input_sequences.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d62a6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[146.054792, 0.0, 0.0, ..., False, False, True],\n",
       "       [150.981065, 0.0, 0.0, ..., False, False, True],\n",
       "       [215.603282, 0.0, 0.0, ..., False, False, True],\n",
       "       ...,\n",
       "       [122.219937, 0.0, 0.0, ..., False, False, True],\n",
       "       [133.77160644000003, 0.0, 0.0, ..., False, False, True],\n",
       "       [145.03873, 0.0, 0.0, ..., False, False, True]], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14d07090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_errors(models, input_test, output_test):\n",
    "    errors = []\n",
    "    for i in range(24):\n",
    "        predictions = models[f'model_{i}'].predict(input_test)\n",
    "        error = np.mean(abs(predictions - output_test[:, i]))\n",
    "        print(error)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b784660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "23.330939093664878\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "26.48302431581375\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "26.95565026227548\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "27.554132477664037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "29.7242988555987\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "30.814328447538703\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "30.290201356939654\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "26.465867293495645\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "25.571937972229886\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "23.982421413575903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "22.402471118801763\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "23.460715791241842\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "24.040792386022453\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "25.14213050699418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "25.8188348939653\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "25.563197879450037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "26.347938190219683\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "25.3740510620283\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "24.878629256556863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "21.85022346207792\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "17.696898399577492\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "14.477458795954288\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "11.29216871842911\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "1.4357911862967407\n",
      "Overall mean of the MAEs: 23.37308763068386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "overall_mean = mean_absolute_errors(models, input_val, output_sequences)\n",
    "print(f\"Overall mean of the MAEs: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e563fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 17280)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b958418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 17280)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08c3d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 24)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e5941a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_forecasting(model, input_data, steps=24):\n",
    "    future_forecast = []\n",
    "    current_input = list(input_data)\n",
    "\n",
    "    for i in range(steps):\n",
    "        forecasted_val = model.predict([current_input])[0]\n",
    "        future_forecast.append(forecasted_val)\n",
    "\n",
    "        # Prepare new input for the next prediction\n",
    "        current_input.pop(0)  # Remove the oldest lag\n",
    "        current_input.append(forecasted_val)  # Append the newly predicted value\n",
    "\n",
    "    return future_forecast\n",
    "\n",
    "# Use the last known data (from the test set) to start the iterative forecasting\n",
    "starting_data = X_test.iloc[0].values\n",
    "forecast = iterative_forecasting(model, starting_data, 24)\n",
    "\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"n_estimators\": 1000,\n",
    "        \"verbosity\": -1,\n",
    "        \"bagging_freq\": 1,\n",
    "        'device': 'gpu',\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    rmse = mean_absolute_error(Y_val, predictions)\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best RMSE:', study.best_value)\n",
    "\n",
    "Best hyperparameters: {'learning_rate': 0.015247440377194395, 'num_leaves': 13, 'subsample': 0.13740858380047208, 'colsample_bytree': 0.4167953910212117, 'min_data_in_leaf': 15}\n",
    "Best RMSE: 0.5582819486587627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lgb_importances(model, plot=False, num=10):\n",
    "    gain = model.feature_importances_   # changed this line\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'feature': model.booster_.feature_name(),  # changed this line\n",
    "        'gain': 100 * gain / gain.sum()\n",
    "    }).sort_values('gain', ascending=False)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.barplot(x='gain', y='feature', data=feat_imp.head(num))\n",
    "        plt.title('Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(feat_imp.head(num))\n",
    "\n",
    "plot_lgb_importances(model, num=20, plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mae(Y_val, predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs(Y_val-predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_val)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train)\n",
    "np.mean(abs(Y_train-predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
