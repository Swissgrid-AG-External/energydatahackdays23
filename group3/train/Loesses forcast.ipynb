{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802de815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/train_group3/Ying_Alex\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0471c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, header_row=0):\n",
    "    data = pd.read_csv(path, header=header_row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c307ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(df):\n",
    "    # Assuming your data is a pandas DataFrame\n",
    "    has_nan = df.isna().any(axis=1)  # Check for NaN values row-wise\n",
    "    nan_indices = has_nan[has_nan].index  # Get the indices where NaN values exist\n",
    "\n",
    "    if not nan_indices.empty:\n",
    "        print(\"There are NaN values in the data.\")\n",
    "        print(\"Indices of NaN values:\", nan_indices)\n",
    "    else:\n",
    "        print(\"There are no NaN values in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3169252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_resolution(df):\n",
    "    #To identify which data in your DataFrame is at 6-hour resolution and which is at 1-hour resolution\n",
    "    # Calculate the time differences between consecutive timestamps\n",
    "    time_diffs = df.index.to_series().diff()\n",
    "    print(time_diffs)\n",
    "\n",
    "    # Identify the timestamps where the time difference is 6 hours (21600 seconds)\n",
    "    six_hour_resolution_indices = time_diffs[time_diffs == pd.Timedelta(hours=6)].index\n",
    "\n",
    "    # Print the indices where 6-hour resolution data is present\n",
    "    print(\"Indices of 6-hour resolution data:\")\n",
    "    print(six_hour_resolution_indices)\n",
    "    start_time = six_hour_resolution_indices.min()\n",
    "    end_time = six_hour_resolution_indices.max()\n",
    "    return start_time, end_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9324fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_resolution(start_time, end_time, data):\n",
    "    # Create a new DataFrame with a DateTime index at 1-hour intervals\n",
    "    one_hour_index = pd.date_range(start=start_time, end=end_time, freq='1H')\n",
    "    one_hour_index.name = 'datetime'\n",
    "    one_hour_data = pd.DataFrame(index=one_hour_index)\n",
    "\n",
    "    # Use reindex to align the 6-hour resolution data with the 1-hour index\n",
    "    resampled_data = data.reindex(one_hour_index)\n",
    "    \n",
    "\n",
    "    # Use linear interpolation to fill in the missing values\n",
    "    for col in data.columns:\n",
    "        resampled_data[col] = resampled_data[col].interpolate(method='linear')\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "\n",
    "    return resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2a55b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_data_preprocessing(data):\n",
    "    #convert losses from kwh to mwh\n",
    "    data['MWh']=data['kWh']/1000\n",
    "    #convert quarter resolution to hourly resolution\n",
    "    data['Zeitstempel'] = pd.to_datetime(data['Zeitstempel']) \n",
    "    data['time_start'] = data['Zeitstempel']-pd.Timedelta(minutes=15)\n",
    "    data['datetime'] = data['time_start'].apply(lambda x: x.strftime('%Y-%m-%d %H:00:00'))\n",
    "    data['datetime'] = pd.to_datetime(data['datetime']) \n",
    "    data_hour = data.groupby(['datetime'])['MWh'].sum().reset_index()\n",
    "    data_hour.set_index('datetime', inplace=True)\n",
    "    return data_hour\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ac2dea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_data_preprocessing(data):\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    data.set_index('datetime', inplace=True)\n",
    "    check_nan(data)\n",
    "    # get the time period for the data at 6 hour resolution\n",
    "    start_time, end_time = check_resolution(temp)\n",
    "    start_time = start_time - pd.Timedelta(hours=6)\n",
    "    # resample the data at 6 hour resolution to 1 hour resolution\n",
    "    resampled_data = convert_resolution(start_time, end_time, temp[start_time:end_time])\n",
    "    start_time_for_rest = end_time + pd.Timedelta(hours=1)\n",
    "    original_data = temp[start_time_for_rest:]\n",
    "    # add first row for datetime 2019-01-01 00:00:00\n",
    "    first_row_index = resampled_data.index[0] - pd.Timedelta(hours=1)\n",
    "    first_row_data = resampled_data.iloc[0].values\n",
    "    first_row = pd.DataFrame([first_row_data], columns=resampled_data.columns, index=[first_row_index])\n",
    "    frist_part = pd.concat([first_row, resampled_data])\n",
    "    print(frist_part.index)\n",
    "    print(len(frist_part.index))\n",
    "    print(len(frist_part.index.unique()))\n",
    "    #concatenate all data together\n",
    "    total = pd.concat([frist_part, original_data])\n",
    "\n",
    "    # add row for datetime 2021-03-28 02:00:00\n",
    "    row_index = pd.to_datetime('2021-03-28 02:00:00')\n",
    "    row_before_index = row_index - pd.Timedelta(hours=1)\n",
    "    row_before_data = total.loc[row_before_index].values\n",
    "    row = pd.DataFrame([row_before_data], columns=resampled_data.columns, index=[row_index])\n",
    "    total = pd.concat([total, row])\n",
    "    print(total.index)\n",
    "    print(len(total.index))\n",
    "    print(len(total.index.unique()))\n",
    "    # remove one entry for datetime 2021-10-31 02:00:00\n",
    "    \n",
    "    total = total[~total.index.duplicated(keep='first')]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c25481b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntc_datapreprocessing(data, full_index):\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    data.set_index('datetime', inplace=True)\n",
    "    # Check for duplicate indices\n",
    "#     duplicate_indices = data.index[data.index.duplicated()]\n",
    "    data = fill_missing_data(data, full_index)\n",
    "    data = data[~data.index.duplicated(keep='first')]\n",
    "    return data\n",
    "#     print(data.index)\n",
    "#     print(len(data.index))\n",
    "#     print(len(data.index.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4ffa3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26301\n",
      "2019-03-31 02:00:00\n",
      "2019-03-31 01:00:00\n",
      "[ 650. 4000. 1200. 2360. 1136. 1800. 3000. 1910.]\n",
      "2020-03-29 02:00:00\n",
      "2020-03-29 01:00:00\n",
      "[1200. 4000. 1200. 1071. 1200.  800. 3000. 1910.]\n",
      "2021-03-28 02:00:00\n",
      "2021-03-28 01:00:00\n",
      "[1200. 4000. 1400. 3560.  900.  800. 2385. 1910.]\n",
      "                      CH_AT   CH_DE   CH_FR   CH_IT   AT_CH   DE_CH   FR_CH  \\\n",
      "2019-01-01 00:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 01:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 02:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 03:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 04:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "...                     ...     ...     ...     ...     ...     ...     ...   \n",
      "2021-12-31 22:00:00  1200.0  4000.0  1400.0  3780.0  1200.0   800.0  3200.0   \n",
      "2021-12-31 23:00:00  1200.0  4000.0  1400.0  2992.0  1200.0   800.0  3200.0   \n",
      "2019-03-31 02:00:00   650.0  4000.0  1200.0  2360.0  1136.0  1800.0  3000.0   \n",
      "2020-03-29 02:00:00  1200.0  4000.0  1200.0  1071.0  1200.0   800.0  3000.0   \n",
      "2021-03-28 02:00:00  1200.0  4000.0  1400.0  3560.0   900.0   800.0  2385.0   \n",
      "\n",
      "                      IT_CH  \n",
      "2019-01-01 00:00:00  1910.0  \n",
      "2019-01-01 01:00:00  1910.0  \n",
      "2019-01-01 02:00:00  1910.0  \n",
      "2019-01-01 03:00:00  1910.0  \n",
      "2019-01-01 04:00:00  1910.0  \n",
      "...                     ...  \n",
      "2021-12-31 22:00:00  1810.0  \n",
      "2021-12-31 23:00:00  1910.0  \n",
      "2019-03-31 02:00:00  1910.0  \n",
      "2020-03-29 02:00:00  1910.0  \n",
      "2021-03-28 02:00:00  1910.0  \n",
      "\n",
      "[26304 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "ntc_path = '/home/jupyter-user_13/data/NTC.csv'\n",
    "ntc = get_data(ntc_path)\n",
    "print(len(ntc['datetime'].unique()))\n",
    "full_index = losses.index\n",
    "ntc = ntc_datapreprocessing(ntc, full_index)\n",
    "print(ntc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b2f1d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(df, full_index):\n",
    "    for index in full_index:\n",
    "        if index not in df.index:\n",
    "            print(index)\n",
    "            row_before_index = index - pd.Timedelta(hours=1)\n",
    "            print(row_before_index)\n",
    "            row_before_data = df.loc[row_before_index].values\n",
    "            print(row_before_data)\n",
    "            row = pd.DataFrame([row_before_data], columns=df.columns, index=[index])\n",
    "            df = pd.concat([df, row])\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b662356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df1, df2):\n",
    "    print(len(df1.index.unique()))\n",
    "    print(len(df2.index.unique()))\n",
    "        # Check if the indices of df1 and df2 are exactly the same\n",
    "    if df1.index.equals(df2.index):\n",
    "        print(\"The indices are exactly the same.\")\n",
    "    else:\n",
    "        print(\"The indices are not the same.\")\n",
    "    print(df1)\n",
    "    print(df2)\n",
    "    df_merge = df1.join(df2)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dde60278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the data.\n",
      "datetime\n",
      "2019-01-01 01:00:00               NaT\n",
      "2019-01-01 07:00:00   0 days 06:00:00\n",
      "2019-01-01 13:00:00   0 days 06:00:00\n",
      "2019-01-01 19:00:00   0 days 06:00:00\n",
      "2019-01-02 01:00:00   0 days 06:00:00\n",
      "                            ...      \n",
      "2021-12-31 19:00:00   0 days 01:00:00\n",
      "2021-12-31 20:00:00   0 days 01:00:00\n",
      "2021-12-31 21:00:00   0 days 01:00:00\n",
      "2021-12-31 22:00:00   0 days 01:00:00\n",
      "2021-12-31 23:00:00   0 days 01:00:00\n",
      "Name: datetime, Length: 11363, dtype: timedelta64[ns]\n",
      "Indices of 6-hour resolution data:\n",
      "DatetimeIndex(['2019-01-01 07:00:00', '2019-01-01 13:00:00',\n",
      "               '2019-01-01 19:00:00', '2019-01-02 01:00:00',\n",
      "               '2019-01-02 07:00:00', '2019-01-02 13:00:00',\n",
      "               '2019-01-02 19:00:00', '2019-01-03 01:00:00',\n",
      "               '2019-01-03 07:00:00', '2019-01-03 13:00:00',\n",
      "               ...\n",
      "               '2021-01-14 19:00:00', '2021-01-15 01:00:00',\n",
      "               '2021-01-15 07:00:00', '2021-01-15 13:00:00',\n",
      "               '2021-01-15 19:00:00', '2021-01-16 01:00:00',\n",
      "               '2021-01-16 07:00:00', '2021-01-16 13:00:00',\n",
      "               '2021-01-16 19:00:00', '2021-01-17 01:00:00'],\n",
      "              dtype='datetime64[ns]', name='datetime', length=2984, freq=None)\n",
      "DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 01:00:00',\n",
      "               '2019-01-01 02:00:00', '2019-01-01 03:00:00',\n",
      "               '2019-01-01 04:00:00', '2019-01-01 05:00:00',\n",
      "               '2019-01-01 06:00:00', '2019-01-01 07:00:00',\n",
      "               '2019-01-01 08:00:00', '2019-01-01 09:00:00',\n",
      "               ...\n",
      "               '2021-01-16 16:00:00', '2021-01-16 17:00:00',\n",
      "               '2021-01-16 18:00:00', '2021-01-16 19:00:00',\n",
      "               '2021-01-16 20:00:00', '2021-01-16 21:00:00',\n",
      "               '2021-01-16 22:00:00', '2021-01-16 23:00:00',\n",
      "               '2021-01-17 00:00:00', '2021-01-17 01:00:00'],\n",
      "              dtype='datetime64[ns]', length=17930, freq=None)\n",
      "17930\n",
      "17930\n",
      "DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 01:00:00',\n",
      "               '2019-01-01 02:00:00', '2019-01-01 03:00:00',\n",
      "               '2019-01-01 04:00:00', '2019-01-01 05:00:00',\n",
      "               '2019-01-01 06:00:00', '2019-01-01 07:00:00',\n",
      "               '2019-01-01 08:00:00', '2019-01-01 09:00:00',\n",
      "               ...\n",
      "               '2021-12-31 15:00:00', '2021-12-31 16:00:00',\n",
      "               '2021-12-31 17:00:00', '2021-12-31 18:00:00',\n",
      "               '2021-12-31 19:00:00', '2021-12-31 20:00:00',\n",
      "               '2021-12-31 21:00:00', '2021-12-31 22:00:00',\n",
      "               '2021-12-31 23:00:00', '2021-03-28 02:00:00'],\n",
      "              dtype='datetime64[ns]', length=26305, freq=None)\n",
      "26305\n",
      "26304\n"
     ]
    }
   ],
   "source": [
    "temperature_path ='/home/jupyter-user_13/data/Forecast-temperature_new.csv'\n",
    "temp = get_data(temperature_path)\n",
    "\n",
    "temp = temperature_data_preprocessing(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e723d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            MWh\n",
      "datetime                       \n",
      "2019-01-01 00:00:00  139.525004\n",
      "2019-01-01 01:00:00  129.716036\n",
      "2019-01-01 02:00:00  133.398074\n",
      "2019-01-01 03:00:00  135.133852\n",
      "2019-01-01 04:00:00  131.699424\n",
      "...                         ...\n",
      "2021-12-31 19:00:00  171.707318\n",
      "2021-12-31 20:00:00  159.462903\n",
      "2021-12-31 21:00:00  155.109520\n",
      "2021-12-31 22:00:00  171.370277\n",
      "2021-12-31 23:00:00  146.054791\n",
      "\n",
      "[26304 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "losses_path = '/home/jupyter-user_13/data/Avtice-losses.csv'\n",
    "losses = get_data(losses_path, 1)\n",
    "losses = losses_data_preprocessing(losses)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "971568e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 02:00:00\n",
      "2020-03-29 02:00:00\n",
      "2021-03-28 02:00:00\n"
     ]
    }
   ],
   "source": [
    "for index in losses.index:\n",
    "    if index not in ntc.index:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2c9c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(losses.index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9b9e8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26304\n",
      "26304\n",
      "The indices are not the same.\n",
      "                            MWh\n",
      "datetime                       \n",
      "2019-01-01 00:00:00  139.525004\n",
      "2019-01-01 01:00:00  129.716036\n",
      "2019-01-01 02:00:00  133.398074\n",
      "2019-01-01 03:00:00  135.133852\n",
      "2019-01-01 04:00:00  131.699424\n",
      "...                         ...\n",
      "2021-12-31 19:00:00  171.707318\n",
      "2021-12-31 20:00:00  159.462903\n",
      "2021-12-31 21:00:00  155.109520\n",
      "2021-12-31 22:00:00  171.370277\n",
      "2021-12-31 23:00:00  146.054791\n",
      "\n",
      "[26304 rows x 1 columns]\n",
      "                     temperature_fore_ch  temperature_fore_fr  \\\n",
      "2019-01-01 00:00:00               4.1067             5.972900   \n",
      "2019-01-01 01:00:00               4.1067             5.972900   \n",
      "2019-01-01 02:00:00               3.7155             5.900717   \n",
      "2019-01-01 03:00:00               3.3243             5.828533   \n",
      "2019-01-01 04:00:00               2.9331             5.756350   \n",
      "...                                  ...                  ...   \n",
      "2021-12-31 20:00:00               8.0000             9.400000   \n",
      "2021-12-31 21:00:00               7.5000             8.880000   \n",
      "2021-12-31 22:00:00               6.9700             8.510000   \n",
      "2021-12-31 23:00:00               6.7900             8.220000   \n",
      "2021-03-28 02:00:00               2.3000             5.490000   \n",
      "\n",
      "                     temperature_fore_de  temperature_fore_it  \n",
      "2019-01-01 00:00:00             7.426800             4.028100  \n",
      "2019-01-01 01:00:00             7.426800             4.028100  \n",
      "2019-01-01 02:00:00             7.246083             3.811433  \n",
      "2019-01-01 03:00:00             7.065367             3.594767  \n",
      "2019-01-01 04:00:00             6.884650             3.378100  \n",
      "...                                  ...                  ...  \n",
      "2021-12-31 20:00:00            10.630000             9.110000  \n",
      "2021-12-31 21:00:00            10.510000             8.670000  \n",
      "2021-12-31 22:00:00            10.320000             8.140000  \n",
      "2021-12-31 23:00:00            10.230000             7.670000  \n",
      "2021-03-28 02:00:00             3.310000             8.610000  \n",
      "\n",
      "[26304 rows x 4 columns]\n",
      "26304\n",
      "26304\n",
      "The indices are not the same.\n",
      "                            MWh  temperature_fore_ch  temperature_fore_fr  \\\n",
      "datetime                                                                    \n",
      "2019-01-01 00:00:00  139.525004               4.1067             5.972900   \n",
      "2019-01-01 01:00:00  129.716036               4.1067             5.972900   \n",
      "2019-01-01 02:00:00  133.398074               3.7155             5.900717   \n",
      "2019-01-01 03:00:00  135.133852               3.3243             5.828533   \n",
      "2019-01-01 04:00:00  131.699424               2.9331             5.756350   \n",
      "...                         ...                  ...                  ...   \n",
      "2021-12-31 19:00:00  171.707318               8.5300             9.960000   \n",
      "2021-12-31 20:00:00  159.462903               8.0000             9.400000   \n",
      "2021-12-31 21:00:00  155.109520               7.5000             8.880000   \n",
      "2021-12-31 22:00:00  171.370277               6.9700             8.510000   \n",
      "2021-12-31 23:00:00  146.054791               6.7900             8.220000   \n",
      "\n",
      "                     temperature_fore_de  temperature_fore_it  \n",
      "datetime                                                       \n",
      "2019-01-01 00:00:00             7.426800             4.028100  \n",
      "2019-01-01 01:00:00             7.426800             4.028100  \n",
      "2019-01-01 02:00:00             7.246083             3.811433  \n",
      "2019-01-01 03:00:00             7.065367             3.594767  \n",
      "2019-01-01 04:00:00             6.884650             3.378100  \n",
      "...                                  ...                  ...  \n",
      "2021-12-31 19:00:00            10.790000             9.590000  \n",
      "2021-12-31 20:00:00            10.630000             9.110000  \n",
      "2021-12-31 21:00:00            10.510000             8.670000  \n",
      "2021-12-31 22:00:00            10.320000             8.140000  \n",
      "2021-12-31 23:00:00            10.230000             7.670000  \n",
      "\n",
      "[26304 rows x 5 columns]\n",
      "                      CH_AT   CH_DE   CH_FR   CH_IT   AT_CH   DE_CH   FR_CH  \\\n",
      "2019-01-01 00:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 01:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 02:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 03:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "2019-01-01 04:00:00   700.0  4000.0  1200.0  2513.0  1200.0   800.0  3000.0   \n",
      "...                     ...     ...     ...     ...     ...     ...     ...   \n",
      "2021-12-31 22:00:00  1200.0  4000.0  1400.0  3780.0  1200.0   800.0  3200.0   \n",
      "2021-12-31 23:00:00  1200.0  4000.0  1400.0  2992.0  1200.0   800.0  3200.0   \n",
      "2019-03-31 02:00:00   650.0  4000.0  1200.0  2360.0  1136.0  1800.0  3000.0   \n",
      "2020-03-29 02:00:00  1200.0  4000.0  1200.0  1071.0  1200.0   800.0  3000.0   \n",
      "2021-03-28 02:00:00  1200.0  4000.0  1400.0  3560.0   900.0   800.0  2385.0   \n",
      "\n",
      "                      IT_CH  \n",
      "2019-01-01 00:00:00  1910.0  \n",
      "2019-01-01 01:00:00  1910.0  \n",
      "2019-01-01 02:00:00  1910.0  \n",
      "2019-01-01 03:00:00  1910.0  \n",
      "2019-01-01 04:00:00  1910.0  \n",
      "...                     ...  \n",
      "2021-12-31 22:00:00  1810.0  \n",
      "2021-12-31 23:00:00  1910.0  \n",
      "2019-03-31 02:00:00  1910.0  \n",
      "2020-03-29 02:00:00  1910.0  \n",
      "2021-03-28 02:00:00  1910.0  \n",
      "\n",
      "[26304 rows x 8 columns]\n",
      "                            MWh  temperature_fore_ch  temperature_fore_fr  \\\n",
      "datetime                                                                    \n",
      "2019-01-01 00:00:00  139.525004               4.1067             5.972900   \n",
      "2019-01-01 01:00:00  129.716036               4.1067             5.972900   \n",
      "2019-01-01 02:00:00  133.398074               3.7155             5.900717   \n",
      "2019-01-01 03:00:00  135.133852               3.3243             5.828533   \n",
      "2019-01-01 04:00:00  131.699424               2.9331             5.756350   \n",
      "...                         ...                  ...                  ...   \n",
      "2021-12-31 19:00:00  171.707318               8.5300             9.960000   \n",
      "2021-12-31 20:00:00  159.462903               8.0000             9.400000   \n",
      "2021-12-31 21:00:00  155.109520               7.5000             8.880000   \n",
      "2021-12-31 22:00:00  171.370277               6.9700             8.510000   \n",
      "2021-12-31 23:00:00  146.054791               6.7900             8.220000   \n",
      "\n",
      "                     temperature_fore_de  temperature_fore_it   CH_AT   CH_DE  \\\n",
      "datetime                                                                        \n",
      "2019-01-01 00:00:00             7.426800             4.028100   700.0  4000.0   \n",
      "2019-01-01 01:00:00             7.426800             4.028100   700.0  4000.0   \n",
      "2019-01-01 02:00:00             7.246083             3.811433   700.0  4000.0   \n",
      "2019-01-01 03:00:00             7.065367             3.594767   700.0  4000.0   \n",
      "2019-01-01 04:00:00             6.884650             3.378100   700.0  4000.0   \n",
      "...                                  ...                  ...     ...     ...   \n",
      "2021-12-31 19:00:00            10.790000             9.590000  1200.0  4000.0   \n",
      "2021-12-31 20:00:00            10.630000             9.110000  1200.0  4000.0   \n",
      "2021-12-31 21:00:00            10.510000             8.670000  1200.0  4000.0   \n",
      "2021-12-31 22:00:00            10.320000             8.140000  1200.0  4000.0   \n",
      "2021-12-31 23:00:00            10.230000             7.670000  1200.0  4000.0   \n",
      "\n",
      "                      CH_FR   CH_IT   AT_CH  DE_CH   FR_CH   IT_CH  \n",
      "datetime                                                            \n",
      "2019-01-01 00:00:00  1200.0  2513.0  1200.0  800.0  3000.0  1910.0  \n",
      "2019-01-01 01:00:00  1200.0  2513.0  1200.0  800.0  3000.0  1910.0  \n",
      "2019-01-01 02:00:00  1200.0  2513.0  1200.0  800.0  3000.0  1910.0  \n",
      "2019-01-01 03:00:00  1200.0  2513.0  1200.0  800.0  3000.0  1910.0  \n",
      "2019-01-01 04:00:00  1200.0  2513.0  1200.0  800.0  3000.0  1910.0  \n",
      "...                     ...     ...     ...    ...     ...     ...  \n",
      "2021-12-31 19:00:00  1400.0  4069.0  1200.0  800.0  3200.0  1810.0  \n",
      "2021-12-31 20:00:00  1400.0  4069.0  1200.0  800.0  3200.0  1810.0  \n",
      "2021-12-31 21:00:00  1400.0  3953.0  1200.0  800.0  3200.0  1810.0  \n",
      "2021-12-31 22:00:00  1400.0  3780.0  1200.0  800.0  3200.0  1810.0  \n",
      "2021-12-31 23:00:00  1400.0  2992.0  1200.0  800.0  3200.0  1910.0  \n",
      "\n",
      "[26304 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge losses and temperature\n",
    "merged = merge_data(losses, temp)\n",
    "merged = merge_data(merged, ntc)\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52a82cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c7185b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date ='2019-01-01 00'\n",
    "train_end_date ='2021-09-30 23'\n",
    "test_start_date ='2021-10-01 00'\n",
    "test_end_date ='2021-12-31 23'\n",
    "\n",
    "\n",
    "train_data = merged.loc[train_start_date : train_end_date]\n",
    "test_data = merged.loc[test_start_date : test_end_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "53cff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input data\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data)-seq_length-24):\n",
    "        sequences.append(data.iloc[i:i+seq_length])\n",
    "        targets.append(data.iloc[(i+seq_length):(i+seq_length+24),0])\n",
    "\n",
    "    sequence_array = np.array(sequences)\n",
    "    targets_array = np.array(targets)\n",
    "    return sequence_array, targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b369c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Scale the input data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "seq_length = 24*7\n",
    "\n",
    "X_train,y_train = create_sequences(train_data, seq_length)\n",
    "X_test,y_test = create_sequences(test_data, seq_length)\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3342f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Scale the input data\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "seq_length = 24*7\n",
    "\n",
    "X_train,y_train = create_sequences(train_data, seq_length)\n",
    "X_test,y_test = create_sequences(test_data, seq_length)\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.fit_transform(X_test)\n",
    "scaler_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f0fc120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23904, 168, 5) (23904, 24) (2016, 168, 5) (2016, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "34b3662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7fe910c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23904, 840) (23904, 24) (2016, 840) (2016, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8781256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the target data to make it compatible with the model's output\n",
    "# y_train = y_train.reshape(-1, 24)\n",
    "# y_test = y_test.reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7572fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23904, 840) (23904, 24) (2016, 840) (2016, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca96121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1])\n",
    "# y_train = y_train.reshape(y_train.shape[0], y_train.shape[1])\n",
    "# x_test = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "# y_test = y_test.reshape(y_test.shape[0], y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421331a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/70, Loss: 765.8552499862916\n",
      "Epoch 2/70, Loss: 600.1794930524367\n",
      "Epoch 3/70, Loss: 568.0573679062135\n",
      "Epoch 4/70, Loss: 557.9007893118629\n",
      "Epoch 5/70, Loss: 544.4130713314934\n",
      "Epoch 6/70, Loss: 543.5922348920037\n",
      "Epoch 7/70, Loss: 534.4827181566208\n",
      "Epoch 8/70, Loss: 539.1458074396306\n",
      "Epoch 9/70, Loss: 530.3591009129815\n",
      "Epoch 10/70, Loss: 519.3069537973658\n",
      "Epoch 11/70, Loss: 519.4712660682393\n",
      "Epoch 12/70, Loss: 519.6356202803831\n",
      "Epoch 13/70, Loss: 520.2967575807622\n",
      "Epoch 14/70, Loss: 513.8223208667123\n",
      "Epoch 15/70, Loss: 509.61578287542824\n",
      "Epoch 16/70, Loss: 504.5006361364681\n",
      "Epoch 17/70, Loss: 505.15217027307193\n",
      "Epoch 18/70, Loss: 501.4291784929082\n",
      "Epoch 19/70, Loss: 494.5894559972426\n",
      "Epoch 20/70, Loss: 498.583291201668\n",
      "Epoch 21/70, Loss: 500.77223776751026\n",
      "Epoch 22/70, Loss: 490.7286851036357\n",
      "Epoch 23/70, Loss: 487.1821502032764\n",
      "Epoch 24/70, Loss: 489.4169766023197\n",
      "Epoch 25/70, Loss: 484.3865172850257\n",
      "Epoch 26/70, Loss: 484.68082073028074\n",
      "Epoch 27/70, Loss: 484.90294634977124\n",
      "Epoch 28/70, Loss: 483.91180721833746\n",
      "Epoch 29/70, Loss: 481.65672416483017\n",
      "Epoch 30/70, Loss: 474.27713722596195\n",
      "Epoch 31/70, Loss: 473.9573219013724\n",
      "Epoch 32/70, Loss: 474.5087368399064\n",
      "Epoch 33/70, Loss: 472.50769785508754\n",
      "Epoch 34/70, Loss: 475.6056103221873\n",
      "Epoch 35/70, Loss: 474.6061440534133\n",
      "Epoch 36/70, Loss: 469.03187552875374\n",
      "Epoch 37/70, Loss: 467.81664753470193\n",
      "Epoch 38/70, Loss: 465.67272524910175\n",
      "Epoch 39/70, Loss: 466.6253127643769\n",
      "Epoch 40/70, Loss: 468.235161113229\n",
      "Epoch 41/70, Loss: 463.9187498857631\n",
      "Epoch 42/70, Loss: 464.81581384995405\n",
      "Epoch 43/70, Loss: 463.4524521445208\n",
      "Epoch 44/70, Loss: 462.0487993209757\n",
      "Epoch 45/70, Loss: 463.7172647567994\n",
      "Epoch 46/70, Loss: 460.71214677943266\n",
      "Epoch 47/70, Loss: 461.1883800322997\n",
      "Epoch 48/70, Loss: 458.6233847715\n",
      "Epoch 49/70, Loss: 460.10733170942825\n",
      "Epoch 50/70, Loss: 458.6282563235033\n",
      "Epoch 51/70, Loss: 457.71185686244047\n",
      "Epoch 52/70, Loss: 457.42116942380204\n",
      "Epoch 53/70, Loss: 456.2105020125282\n",
      "Epoch 54/70, Loss: 456.63249084146264\n",
      "Epoch 55/70, Loss: 459.93548608463715\n",
      "Epoch 56/70, Loss: 455.79461702560997\n",
      "Epoch 57/70, Loss: 454.51204709955715\n",
      "Epoch 58/70, Loss: 451.869093951057\n",
      "Epoch 59/70, Loss: 455.12398826884714\n",
      "Epoch 60/70, Loss: 454.73573246104195\n",
      "Epoch 61/70, Loss: 451.7255129890646\n",
      "Epoch 62/70, Loss: 450.0485648088914\n",
      "Epoch 63/70, Loss: 449.20809846765854\n",
      "Epoch 64/70, Loss: 449.77194703454\n",
      "Epoch 65/70, Loss: 447.49570885316575\n",
      "Epoch 66/70, Loss: 449.4954643045517\n",
      "Epoch 67/70, Loss: 447.3407387146975\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRegressionModel, self).__init__()\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(168 * 5, 128)  # Fully connected layer 1\n",
    "        self.relu1 = nn.ReLU()              # ReLU activation\n",
    "        self.fc2 = nn.Linear(128, 64)       # Fully connected layer 2\n",
    "        self.relu2 = nn.ReLU()              # ReLU activation\n",
    "        self.fc3 = nn.Linear(64, 24)        # Output layer (24 output neurons for regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass through the layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    def calculate_mae(self,y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Mean Absolute Error (MAE) between true and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (torch.Tensor): True target values.\n",
    "            y_pred (torch.Tensor): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Calculated MAE.\n",
    "        \"\"\"\n",
    "        absolute_errors = torch.abs(y_true - y_pred)\n",
    "        mae = torch.mean(absolute_errors)\n",
    "        return mae\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to PyTorch tensors and create datasets and dataloaders\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train).to(device)\n",
    "y_train_tensor = torch.Tensor(y_train).to(device)\n",
    "X_test_tensor = torch.Tensor(X_test).to(device)\n",
    "y_test_tensor = torch.Tensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create an instance of the regression model\n",
    "model = MyRegressionModel().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "# print(regression_model)\n",
    "# Define the loss function (mean squared error) and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 70\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Assuming 'model' is your PyTorch model\n",
    "model_path = 'my_model.pth'  # Specify the path where you want to save the model\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    # Calculate MAE during training (you can also do this for validation)\n",
    "    mae = model.calculate_mae(y_test_tensor,y_pred)\n",
    "    print(f\"Epoch {epoch + 1}, Batch MAE: {mae.item()}\")\n",
    "#     mse = criterion(y_pred, y_test_tensor).item()\n",
    "#     rmse = np.sqrt(mse)\n",
    "\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "49bcfe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/70, Loss: 1326.283061797606\n",
      "Epoch 2/70, Loss: 678.556767427985\n",
      "Epoch 3/70, Loss: 655.5813101702196\n",
      "Epoch 4/70, Loss: 642.3698813698509\n",
      "Epoch 5/70, Loss: 636.6701553263129\n",
      "Epoch 6/70, Loss: 624.4741025710489\n",
      "Epoch 7/70, Loss: 625.4180011443276\n",
      "Epoch 8/70, Loss: 620.6155681100121\n",
      "Epoch 9/70, Loss: 614.0992363098471\n",
      "Epoch 10/70, Loss: 616.3429489543731\n",
      "Epoch 11/70, Loss: 610.4295145942566\n",
      "Epoch 12/70, Loss: 603.8940141647257\n",
      "Epoch 13/70, Loss: 595.0145363221194\n",
      "Epoch 14/70, Loss: 587.4844712854069\n",
      "Epoch 15/70, Loss: 576.1510704978903\n",
      "Epoch 16/70, Loss: 572.9017796643915\n",
      "Epoch 17/70, Loss: 562.4102448652135\n",
      "Epoch 18/70, Loss: 563.2341297986036\n",
      "Epoch 19/70, Loss: 559.579076348779\n",
      "Epoch 20/70, Loss: 560.0663907464175\n",
      "Epoch 21/70, Loss: 551.2220090978286\n",
      "Epoch 22/70, Loss: 546.5443740273541\n",
      "Epoch 23/70, Loss: 542.5317083348565\n",
      "Epoch 24/70, Loss: 539.4178721382019\n",
      "Epoch 25/70, Loss: 538.8653987129742\n",
      "Epoch 26/70, Loss: 538.4528893455465\n",
      "Epoch 27/70, Loss: 530.495128366399\n",
      "Epoch 28/70, Loss: 528.1086125501337\n",
      "Epoch 29/70, Loss: 521.6002816592945\n",
      "Epoch 30/70, Loss: 517.82496398273\n",
      "Epoch 31/70, Loss: 515.2764810164344\n",
      "Epoch 32/70, Loss: 513.3584806064871\n",
      "Epoch 33/70, Loss: 508.62220788639496\n",
      "Epoch 34/70, Loss: 512.5513016807841\n",
      "Epoch 35/70, Loss: 511.92696409174465\n",
      "Epoch 36/70, Loss: 506.9797261283997\n",
      "Epoch 37/70, Loss: 504.61956509676844\n",
      "Epoch 38/70, Loss: 505.0661510936717\n",
      "Epoch 39/70, Loss: 500.6053191812281\n",
      "Epoch 40/70, Loss: 499.4853315710384\n",
      "Epoch 41/70, Loss: 502.3902916729769\n",
      "Epoch 42/70, Loss: 501.22819535362527\n",
      "Epoch 43/70, Loss: 498.99638791007794\n",
      "Epoch 44/70, Loss: 495.36576973818205\n",
      "Epoch 45/70, Loss: 497.3099690809607\n",
      "Epoch 46/70, Loss: 494.7321251037924\n",
      "Epoch 47/70, Loss: 490.03274250540505\n",
      "Epoch 48/70, Loss: 494.24100359748394\n",
      "Epoch 49/70, Loss: 489.65140562006496\n",
      "Epoch 50/70, Loss: 488.2661402085248\n",
      "Epoch 51/70, Loss: 485.63155344590785\n",
      "Epoch 52/70, Loss: 482.3259937469972\n",
      "Epoch 53/70, Loss: 485.66877624185327\n",
      "Epoch 54/70, Loss: 479.2510226652584\n",
      "Epoch 55/70, Loss: 479.7943378795277\n",
      "Epoch 56/70, Loss: 475.66732078185055\n",
      "Epoch 57/70, Loss: 477.47612448299634\n",
      "Epoch 58/70, Loss: 474.84366929977335\n",
      "Epoch 59/70, Loss: 469.8827859807142\n",
      "Epoch 60/70, Loss: 469.95364224846986\n",
      "Epoch 61/70, Loss: 466.2088065733884\n",
      "Epoch 62/70, Loss: 462.27295721533466\n",
      "Epoch 63/70, Loss: 464.541161032284\n",
      "Epoch 64/70, Loss: 462.0180358886719\n",
      "Epoch 65/70, Loss: 462.3389375248057\n",
      "Epoch 66/70, Loss: 460.9400006462546\n",
      "Epoch 67/70, Loss: 456.73435949641754\n",
      "Epoch 68/70, Loss: 457.4620830515489\n",
      "Epoch 69/70, Loss: 454.71062245088467\n",
      "Epoch 70/70, Loss: 453.35707337078566\n",
      "Epoch 70, Batch MAE: 26878.751953125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRegressionModel, self).__init__()\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(168 * 5, 128)  # Fully connected layer 1\n",
    "        self.relu1 = nn.ReLU()              # ReLU activation\n",
    "        self.fc2 = nn.Linear(128, 64)       # Fully connected layer 2\n",
    "        self.relu2 = nn.ReLU()              # ReLU activation\n",
    "        self.fc3 = nn.Linear(64, 24)        # Output layer (24 output neurons for regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass through the layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    def calculate_mae(self,y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Mean Absolute Error (MAE) between true and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (torch.Tensor): True target values.\n",
    "            y_pred (torch.Tensor): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Calculated MAE.\n",
    "        \"\"\"\n",
    "        absolute_errors = np.abs(y_true - y_pred)\n",
    "        mae = np.mean(absolute_errors)\n",
    "        return mae\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to PyTorch tensors and create datasets and dataloaders\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.Tensor(y_train).to(device)\n",
    "X_test_tensor = torch.Tensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.Tensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create an instance of the regression model\n",
    "model = MyRegressionModel().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "# print(regression_model)\n",
    "# Define the loss function (mean squared error) and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 70\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Assuming 'model' is your PyTorch model\n",
    "model_path = 'my_model.pth'  # Specify the path where you want to save the model\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_cpu = y_pred.cpu().detach().numpy()\n",
    "    # Reverse the scaling for y_pred to the original scale\n",
    "    scaler_y = MinMaxScaler()  # Create a new scaler for the output\n",
    "    scaler_y.fit(y_pred_cpu)  # Fit the scaler on the original y data\n",
    "    # Inverse transform the predictions to the original scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_cpu)\n",
    "    # Calculate MAE during training (you can also do this for validation)\n",
    "    mae = model.calculate_mae(y_pred, y_test_tensor.cpu().detach().numpy())\n",
    "    print(f\"Epoch {epoch + 1}, Batch MAE: {mae.item()}\")\n",
    "#     mse = criterion(y_pred, y_test_tensor).item()\n",
    "#     rmse = np.sqrt(mse)\n",
    "\n",
    "# print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99f57d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4bf47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dda90b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRegressionModel(\n",
      "  (conv1): Conv2d(5, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=5120, out_features=128, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "# Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(168 * 5, 128)  # Fully connected layer 1\n",
    "        self.relu1 = nn.ReLU()              # ReLU activation\n",
    "        self.fc2 = nn.Linear(128, 64)       # Fully connected layer 2\n",
    "        self.relu2 = nn.ReLU()              # ReLU activation\n",
    "        self.fc3 = nn.Linear(64, 24)        # Output layer (24 output neurons for regression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d6a89685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/50, Loss: 949.4913760098544\n",
      "Epoch 2/50, Loss: 616.8321649887982\n",
      "Epoch 3/50, Loss: 591.0670209262461\n",
      "Epoch 4/50, Loss: 571.5438948850581\n",
      "Epoch 5/50, Loss: 557.6853449204389\n",
      "Epoch 6/50, Loss: 551.3929916626629\n",
      "Epoch 7/50, Loss: 553.5069704106785\n",
      "Epoch 8/50, Loss: 547.8167818446848\n",
      "Epoch 9/50, Loss: 545.3272486396014\n",
      "Epoch 10/50, Loss: 544.7291768119934\n",
      "Epoch 11/50, Loss: 545.6561182195491\n",
      "Epoch 12/50, Loss: 541.4460505521233\n",
      "Epoch 13/50, Loss: 542.3478499631831\n",
      "Epoch 14/50, Loss: 541.9111430122254\n",
      "Epoch 15/50, Loss: 539.285353308693\n",
      "Epoch 16/50, Loss: 536.9506991789302\n",
      "Epoch 17/50, Loss: 543.0647250088779\n",
      "Epoch 18/50, Loss: 536.5910766927954\n",
      "Epoch 19/50, Loss: 537.4339408670518\n",
      "Epoch 20/50, Loss: 538.2079525707877\n",
      "Epoch 21/50, Loss: 536.3120889918689\n",
      "Epoch 22/50, Loss: 531.6399360697537\n",
      "Epoch 23/50, Loss: 532.0558044107203\n",
      "Epoch 24/50, Loss: 534.7366685510319\n",
      "Epoch 25/50, Loss: 534.703904503807\n",
      "Epoch 26/50, Loss: 532.2325707909895\n",
      "Epoch 27/50, Loss: 531.0279855983142\n",
      "Epoch 28/50, Loss: 530.346092672909\n",
      "Epoch 29/50, Loss: 531.3737266662924\n",
      "Epoch 30/50, Loss: 530.0411524645148\n",
      "Epoch 31/50, Loss: 530.9347684319644\n",
      "Epoch 32/50, Loss: 533.2646788734803\n",
      "Epoch 33/50, Loss: 528.324277826809\n",
      "Epoch 34/50, Loss: 527.0990770309367\n",
      "Epoch 35/50, Loss: 527.235529200916\n",
      "Epoch 36/50, Loss: 526.0113458480427\n",
      "Epoch 37/50, Loss: 525.2259732006705\n",
      "Epoch 38/50, Loss: 527.8868224608069\n",
      "Epoch 39/50, Loss: 524.1433791706269\n",
      "Epoch 40/50, Loss: 524.8818084390406\n",
      "Epoch 41/50, Loss: 524.3034969064641\n",
      "Epoch 42/50, Loss: 524.41627257648\n",
      "Epoch 43/50, Loss: 523.9136322347875\n",
      "Epoch 44/50, Loss: 521.8088263853349\n",
      "Epoch 45/50, Loss: 522.6679755226176\n",
      "Epoch 46/50, Loss: 525.0596667611026\n",
      "Epoch 47/50, Loss: 521.6589499080883\n",
      "Epoch 48/50, Loss: 521.3527647620216\n",
      "Epoch 49/50, Loss: 523.3328238910532\n",
      "Epoch 50/50, Loss: 520.7325611624489\n",
      "Root Mean Squared Error (RMSE): 24.238005198367578\n"
     ]
    }
   ],
   "source": [
    "#Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to PyTorch tensors and create datasets and dataloaders\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train).to(device)\n",
    "y_train_tensor = torch.Tensor(y_train).to(device)\n",
    "X_test_tensor = torch.Tensor(X_test).to(device)\n",
    "y_test_tensor = torch.Tensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Define the regression model\n",
    "# class RegressionModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RegressionModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=168, hidden_size=128, num_layers=2, batch_first=True)\n",
    "#         self.fc = nn.Linear(128, 24)  # 24 is the output dimension\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = self.fc(out[:, -1, :])  # Only take the output at the last time step\n",
    "#         return out\n",
    "    \n",
    "\n",
    "\n",
    "#Create an instance of the model\n",
    "# model = RegressionModel().to(device)\n",
    "# model = RegressionModel()\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Input size to 128 hidden units\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)  # 128 hidden units to 64 hidden units\n",
    "        self.fc3 = nn.Linear(64, 24)  # 64 hidden units to 24 output units (output dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze(dim=1)  # Remove the middle dimension to match target shape\n",
    "\n",
    "input_dim = X_train.shape[2] \n",
    "model = NeuralNetwork(input_dim).to(device)\n",
    "# Define the loss function (mean squared error) and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    mse = criterion(y_pred, y_test_tensor).item()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e806eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
