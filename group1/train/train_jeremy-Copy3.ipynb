{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bc5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "import holidays\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import kurtosis, skew\n",
    "from load_data import load_data, split_data\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b548f22",
   "metadata": {},
   "source": [
    "# Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69700a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "#load data\n",
    "df_ntc = pd.read_csv(\"../data/NTC.csv\", parse_dates=['datetime'])\n",
    "df_ntc.set_index('datetime', inplace=True)\n",
    "df_ntc.index = df_ntc.index.tz_localize('Europe/Brussels', ambiguous='infer')\n",
    "df_ntc.index = df_ntc.index.tz_convert('UTC')\n",
    "\n",
    "df_act_loss = pd.read_csv(\"../data/Avtice-losses.csv\", skiprows=1, parse_dates=['Zeitstempel'], index_col='Zeitstempel')\n",
    "df_act_loss = df_act_loss.rename(columns={\"kWh\": \"mwh\", \"Zeitstempel\": \"datetime\"})\n",
    "df_act_loss.index.name = 'datetime'\n",
    "df_act_loss.index = df_act_loss.index.tz_localize('Europe/Brussels', ambiguous='infer')\n",
    "df_act_loss.index = df_act_loss.index.tz_convert('UTC')\n",
    "\n",
    "df_weather_forecast = pd.read_csv(\"../data/Forecast-renewable-generation.csv\", parse_dates=['datetime'])\n",
    "df_weather_forecast.set_index('datetime', inplace=True)\n",
    "df_weather_forecast.index = df_weather_forecast.index.tz_localize('Europe/Brussels', ambiguous='infer')\n",
    "df_weather_forecast.index = df_weather_forecast.index.tz_convert('UTC')\n",
    "\n",
    "# df_weather_temp = pd.read_csv(\"../data/Forecast-temperature_new.csv\", parse_dates=['datetime'])\n",
    "# df_weather_temp.set_index('datetime', inplace=True)\n",
    "# ambiguous_flags = [False] * len(df_weather_temp)  # Set all to False initially\n",
    "# ambiguous_idx = df_weather_temp.index.get_loc(pd.Timestamp(\"2019-10-27 02:00:00\"))\n",
    "# ambiguous_flags[ambiguous_idx] = True  # Adjust as needed; True means it is DST\n",
    "# df_weather_temp.index = df_weather_temp.index.tz_localize('Europe/Brussels', ambiguous=ambiguous_flags)\n",
    "# df_weather_temp.index = df_weather_temp.index.tz_convert('UTC')\n",
    "\n",
    "\n",
    "# You need to convert kWh to MWh by dividing it by 1000. As all other variables use MW.\n",
    "df_act_loss['mwh'] = df_act_loss['mwh'].astype(float)\n",
    "df_act_loss['mwh'] = df_act_loss['mwh'] / 1000.0\n",
    "\n",
    "#downsample from 15 min to 1 hour\n",
    "df_act_loss = df_act_loss.loc[~df_act_loss.index.duplicated(keep='first')]\n",
    "df_act_loss = df_act_loss.resample('1H').sum()\n",
    "df_act_loss = df_act_loss.interpolate()\n",
    "\n",
    "\n",
    "#display\n",
    "# display(df_ntc)\n",
    "# display(df_act_loss)\n",
    "# display(df_weather_forecast)\n",
    "# display(df_weather_temp)\n",
    "\n",
    "\n",
    "###Merge###\n",
    "# List of DataFrames\n",
    "dfs = [df_ntc, df_weather_forecast, df_act_loss]\n",
    "\n",
    "# Merge all DataFrames on 'datetime'\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='datetime', how='inner'), dfs)\n",
    "\n",
    "# Function to determine the season\n",
    "def determine_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "# Function to determine if the date is a public holiday in Switzerland\n",
    "def is_public_holiday(date):\n",
    "    swiss_holidays = holidays.Switzerland(years=date.year)\n",
    "    return 1 if date in swiss_holidays else 0\n",
    "\n",
    "# Function to determine if the date is a special day in Switzerland (you can define your own special days)\n",
    "def is_special_day(date):\n",
    "    special_days = [pd.Timestamp(f'{date.year}-08-01'), pd.Timestamp(f'{date.year}-12-25'), pd.Timestamp(f'{date.year}-01-01')]\n",
    "    return 1 if date in special_days else 0\n",
    "\n",
    "# Add feature columns\n",
    "def add_feature_columns(df):\n",
    "    # Determine the season\n",
    "    df['Season'] = df.index.month.map(determine_season)\n",
    "    df = pd.get_dummies(df, columns=['Season'], prefix='Season', prefix_sep='')\n",
    "    for col in df.columns:\n",
    "        if 'Season' in col:\n",
    "            df[col] = df[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # Add month feature\n",
    "    df['Month'] = df.index.month\n",
    "    df = pd.get_dummies(df, columns=['Month'], prefix='Month')\n",
    "\n",
    "    # Public Holidays and Special days\n",
    "    df['Public_Holiday'] = df.index.to_series().map(is_public_holiday)\n",
    "    df['Special_Day'] = df.index.to_series().map(is_special_day)\n",
    "\n",
    "    # Add day of the week as a one-hot encoded feature\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    df = pd.get_dummies(df, columns=['Day_of_Week'], prefix='Day')\n",
    "\n",
    "        # Add hour of the day\n",
    "    df['Hour'] = df.index.hour\n",
    "    df = pd.get_dummies(df, columns=['Hour'], prefix='Hour')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming merged_df is your DataFrame with datetime index\n",
    "merged_df = add_feature_columns(merged_df)\n",
    "\n",
    "#clean\n",
    "merged_df.columns = merged_df.columns.str.replace('[\":\\[\\]{}]', '', regex=True)\n",
    "merged_df.columns = [col.replace(\" \", \"_\") for col in merged_df.columns]\n",
    "# display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc794fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CH_AT', 'CH_DE', 'CH_FR', 'CH_IT', 'AT_CH', 'DE_CH', 'FR_CH', 'IT_CH',\n",
       "       'solar_fore_de_MW', 'solar_fore_it_MW', 'wind_fore_de_MW',\n",
       "       'wind_fore_it_MW', 'mwh', 'SeasonAutumn', 'SeasonSpring',\n",
       "       'SeasonSummer', 'SeasonWinter', 'Month_1', 'Month_2', 'Month_3',\n",
       "       'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9',\n",
       "       'Month_10', 'Month_11', 'Month_12', 'Public_Holiday', 'Special_Day',\n",
       "       'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5', 'Day_6', 'Hour_0',\n",
       "       'Hour_1', 'Hour_2', 'Hour_3', 'Hour_4', 'Hour_5', 'Hour_6', 'Hour_7',\n",
       "       'Hour_8', 'Hour_9', 'Hour_10', 'Hour_11', 'Hour_12', 'Hour_13',\n",
       "       'Hour_14', 'Hour_15', 'Hour_16', 'Hour_17', 'Hour_18', 'Hour_19',\n",
       "       'Hour_20', 'Hour_21', 'Hour_22', 'Hour_23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b174807",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1406fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "del merged_df\n",
    "timeframe = 7 * 24\n",
    "\n",
    "df['target'] = df['mwh']\n",
    "feature_columns = ['mwh']\n",
    "\n",
    "\n",
    "for col in feature_columns:\n",
    "    for i in range(1, timeframe + 1):  # Loop over 1 day's worth of hours, adjust as needed\n",
    "        lag_col_name = f'lag_{col}_{i}'  # Construct the lag column name\n",
    "        df[lag_col_name] = df[col].shift(i)  # Initially create lag columns\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            lag_time = idx - pd.Timedelta(hours=i)\n",
    "            # If the date portion of the lag time is the same as the current index, set it to NaN\n",
    "            if lag_time.date() == idx.date():\n",
    "                df.at[idx, lag_col_name] = np.nan\n",
    "        \n",
    "# for col in feature_columns:\n",
    "#     for i in range(1, timeframe + 1):\n",
    "#         lag_col_name = f'lag_{col}_{i}'\n",
    "        \n",
    "#         # Create lag column\n",
    "#         df[lag_col_name] = df[col].shift(i)\n",
    "        \n",
    "#         # Create an index of dates for each lag_time\n",
    "#         lag_time_index = df.index - pd.Timedelta(hours=i)\n",
    "        \n",
    "#         # Set NaN where the date part is the same\n",
    "#         mask = (lag_time_index.date == df.index.date)\n",
    "#         df.loc[mask, lag_col_name] = np.nan\n",
    "\n",
    "# df['std_mwh_24hr'] = df['mwh'].rolling(window=24).std().shift(24)\n",
    "# df['kurtosis_mwh_24hr'] = df['mwh'].rolling(window=24).apply(kurtosis).shift(24)\n",
    "# df['skewness_mwh_24hr'] = df['mwh'].rolling(window=24).apply(skew).shift(24)\n",
    "\n",
    "df = df.iloc[timeframe:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.copy()\n",
    "df.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da68972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Initialize GPU\n",
    "cuda.select_device(0)\n",
    "\n",
    "# Reset GPU\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"mwh\", axis=1)\n",
    "# Sort the DataFrame by datetime\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Train-test split based on timestamp\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Features and target\n",
    "X_train, y_train = train.drop('target', axis=1), train['target']\n",
    "X_test, y_test = test.drop('target', axis=1), test['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12118a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'device': 'gpu',\n",
    "    'random_state': 42,\n",
    "    'gpu_platform_id' : 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "# Create LightGBM data containers\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_round = 50\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bst.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9570e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize JavaScript for SHAP plots\n",
    "shap.initjs()\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(bst)\n",
    "\n",
    "# Calculate SHAP values for test data\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'objective': ['regression'],\n",
    "    'metric': ['mae'],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'num_leaves': [31, 127],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [20, 40],\n",
    "    'device': ['gpu'],\n",
    "    'gpu_platform_id': [0],\n",
    "    'gpu_device_id': [0]\n",
    "}\n",
    "\n",
    "# Create a LightGBM model\n",
    "lgb_estimator = lgb.LGBMRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, \n",
    "                       scoring='neg_mean_absolute_error', cv=3)\n",
    "# Fit the grid search to the data\n",
    "gsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", gsearch.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = gsearch.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
