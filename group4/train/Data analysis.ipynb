{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa43487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "df = pd.read_csv(\"../../data/Avtice-losses.csv\", header = 1, parse_dates = [\"Zeitstempel\"])\n",
    "df[\"Zeitstempel\"] = df[\"Zeitstempel\"] - dt.timedelta(minutes=15)\n",
    "df = df.groupby(\"Zeitstempel\", as_index=False).kWh.mean()\n",
    "df = df.resample(\"H\", on = \"Zeitstempel\").sum().reset_index()\n",
    "data_loss = df.set_index(\"Zeitstempel\")\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_renewable_generation = pd.read_csv(\"../../data/Forecast-renewable-generation.csv\", header = 0, parse_dates = [\"datetime\"])\n",
    "data_renewable_generation = data_renewable_generation.groupby(\"datetime\", as_index=False)[[col for col in data_renewable_generation.columns]].mean()\n",
    "data_renewable_generation = data_renewable_generation.set_index(\"datetime\")\n",
    "data_renewable_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee277cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NTC = pd.read_csv(\"../../data/NTC.csv\", header = 0, parse_dates = [\"datetime\"])\n",
    "data_NTC = data_NTC.groupby(\"datetime\", as_index=False)[[col for col in data_NTC.columns]].mean()\n",
    "data_NTC = data_NTC.set_index(\"datetime\")\n",
    "data_NTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f60498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "cal_fourier_gen_ = CalendarFourier(\"Y\", 6)\n",
    "\n",
    "fourier_data_yearly = cal_fourier_gen_.in_sample(data_loss.index)\n",
    "fourier_data_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import TimeTrend\n",
    "trend_gen = TimeTrend(False, 1)\n",
    "trend = trend_gen.in_sample(data_loss.index)\n",
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e82735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_new = data_loss.join([data_NTC, data_renewable_generation, fourier_data_yearly, trend])\n",
    "data_new = data_loss.join([data_renewable_generation, fourier_data_yearly])\n",
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cols = data_new.columns\n",
    "\n",
    "X = data_new.to_numpy()\n",
    "X = X[[not any(x) for x in np.isnan(X)]]\n",
    "X = pd.DataFrame(X, columns = cols)\n",
    "normalized_X=(X-X.mean())/X.std()\n",
    "train_df, test_df = train_test_split(normalized_X.to_numpy(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e31f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, Tensor, nn\n",
    "import lightning\n",
    "from lightning.pytorch import LightningModule\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "import pickle\n",
    "\n",
    "class SimpleModel(LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        default_dict = {\"epochs\":1, \"batch_size\":2, \"lr\":0.001, \"momentum\": None, \"optimizer\":optim.Adam, \n",
    "                        \"loss\":F.mse_loss, \"metrics\":[MeanSquaredError]}\n",
    "        self.param_dict = {}\n",
    "        #Setting the parameter either from kwrags or by default\n",
    "        for key in default_dict.keys(): \n",
    "            if key in kwargs.keys():\n",
    "                setattr(self, key, kwargs[key])\n",
    "                self.param_dict[key] = kwargs[key]\n",
    "            else:\n",
    "                setattr(self, key, default_dict[key])\n",
    "                self.param_dict[key] = default_dict[key]\n",
    "\n",
    "        # Network Layer\n",
    "        self.l1 = nn.LSTM(17,20,1)\n",
    "        self.l2 = nn.LSTM(20, 8,1)\n",
    "        self.l4 = nn.Linear(8*24*7, 128)\n",
    "        self.l5 = nn.Linear(128,32)\n",
    "        self.l6 = nn.Linear(32,24)\n",
    "\n",
    "        self.activation_elu = nn.ELU()\n",
    "        self.activation_relu = nn.ReLU()\n",
    "        # logs \n",
    "        \n",
    "        # Important: This property activates manual optimization.\n",
    "        # self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.l1(x)[0])\n",
    "        x = nn.ReLU()(self.l2(x)[0])\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = nn.ReLU()(self.l4(x))\n",
    "        x = nn.ReLU()(self.l5(x))\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "\n",
    "        loss = self.loss(y, y_pred)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baeeefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_X, dataset_y, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset_X)-lookback-24):\n",
    "        feature = dataset_X[i:i+lookback]\n",
    "        target = dataset_y[i+lookback: i+lookback+24]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.Tensor(X), torch.Tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8d1fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X, self.y = create_dataset(data, data[:,0], 24*7) \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return  self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95274c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "mydataset = MyDataset(train_df)\n",
    "loader = DataLoader(mydataset, batch_size=batch_size, shuffle=True)\n",
    "print(next(iter(loader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1250f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2023-09-16 13:06:39.860839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-16 13:06:40.698757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/jupyter-user17/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db2b565e1bf45d7bab7a398a53af6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-user17/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "model = SimpleModel(epochs = 10, batch_size = batch_size, lr = 0.0005)\n",
    "model.configure_optimizers()\n",
    "trainer = pl.Trainer(min_epochs= model.epochs, max_epochs=model.epochs, enable_progress_bar=True, enable_model_summary = False) \n",
    "\n",
    "trainer.fit(model= model, train_dataloaders= loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb874888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
